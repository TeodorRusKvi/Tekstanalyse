{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (TFBertForSequenceClassification, BertTokenizer)\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "max_len = 20\n",
    "\n",
    "bert_model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "df = pd.read_csv(url_data + 'new_df.csv')\n",
    "\n",
    "df['All_text'] = df['All_text'].replace(['U.S.', 'U.S.A.'], ['US', 'USA'], regex=True)\n",
    "df['Processed'] = df['Processed'].replace(['U.S.', 'U.S.A.'], ['US', 'USA'], regex=True)\n",
    "df['Processed'] = df['Processed'].fillna(0)\n",
    "df['Processed'] = df['Processed'].astype(str)\n",
    "df['All_text'] = df['All_text'].fillna(0)\n",
    "df['All_text'] = df['All_text'].astype(str)\n",
    "\n",
    "# df.to_csv('new_df.csv', index=False)\n",
    "\n",
    "# Making the relevant columns to lists\n",
    "all_texts = (df['All_text'].to_list())\n",
    "texts = df['Processed'].to_list()\n",
    "\n",
    "# Setting the wanted text for further modelling\n",
    "corpus = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_train_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y_train = y_train_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_train.flatten().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: 'poor people money'\n",
      "input ids: [101, 3532, 2111, 2769, 102]\n",
      "attention mask: [1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "inputs = bert_tokenizer(corpus[3], max_length=max_len, truncation=True, padding=True)\n",
    "\n",
    "print(f'review: \\'{corpus[3]}\\'')\n",
    "print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "print(f'attention mask: {inputs[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encodings(corpus, bert_tokenizer, max_len, trucation=True, padding=True):\n",
    "    return bert_tokenizer(corpus, max_length=max_len, truncation=trucation, padding=padding)\n",
    "    \n",
    "encodings = construct_encodings(corpus, bert_tokenizer, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in encodings: dict_keys(['input_ids', 'attention_mask'])\n",
      "Sample length of input_ids: 12854\n"
     ]
    }
   ],
   "source": [
    "# Check the structure and length\n",
    "print(\"Keys in encodings:\", encodings.keys())\n",
    "print(\"Sample length of input_ids:\", len(encodings['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "tfdataset = construct_tfdataset(encodings, y)\n",
    "\n",
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_size = int(len(corpus) * (1-TEST_SPLIT))\n",
    "\n",
    "tfdataset = tfdataset.shuffle(len(corpus))\n",
    "tfdataset_train = tfdataset.take(train_size)\n",
    "tfdataset_test = tfdataset.skip(train_size)\n",
    "\n",
    "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
    "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import activations, optimizers, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bugat\\anaconda3\\lib\\site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\bugat\\anaconda3\\lib\\site-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tf-keras) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.2.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\bugat\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install tf-keras\n",
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.optimizer_v2 import adam as adam_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m N_EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m TFDistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m adam_v2(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-5\u001b[39m)\n\u001b[0;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "optimizer = adam_v2(learning_rate=3e-5)\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x000002255C78B5D0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-5\u001b[39m)\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m SparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(tfdataset_train, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, epochs\u001b[38;5;241m=\u001b[39mN_EPOCHS)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1504\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[1;32m-> 1504\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1505\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   1506\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m   1507\u001b[0m         metrics\u001b[38;5;241m=\u001b[39mmetrics,\n\u001b[0;32m   1508\u001b[0m         loss_weights\u001b[38;5;241m=\u001b[39mloss_weights,\n\u001b[0;32m   1509\u001b[0m         weighted_metrics\u001b[38;5;241m=\u001b[39mweighted_metrics,\n\u001b[0;32m   1510\u001b[0m         run_eagerly\u001b[38;5;241m=\u001b[39mrun_eagerly,\n\u001b[0;32m   1511\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39msteps_per_execution,\n\u001b[0;32m   1512\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1513\u001b[0m     )\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1515\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1516\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   1517\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1524\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:335\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get(\n\u001b[0;32m    331\u001b[0m         config,\n\u001b[0;32m    332\u001b[0m         use_legacy_optimizer\u001b[38;5;241m=\u001b[39muse_legacy_optimizer,\n\u001b[0;32m    333\u001b[0m     )\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    337\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret optimizer identifier: <keras.src.optimizers.adam.Adam object at 0x000002255C78B5D0>"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "\n",
    "N_EPOCHS = 2\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "optimizer = Adam(learning_rate=3e-5)\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
