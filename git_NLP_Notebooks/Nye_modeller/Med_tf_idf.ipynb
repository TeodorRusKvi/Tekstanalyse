{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "# import wandb\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, Bidirectional, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Concatenate, BatchNormalization, MultiHeadAttention, LayerNormalization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y = y_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'new_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_df.columns:\n",
    "    X_df[col] = X_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1= X_df['without_stopwords']\n",
    "\n",
    "# Konverter kolonnen til et NumPy array\n",
    "X = X_1.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12854.000000\n",
       "mean        47.516104\n",
       "std        182.683952\n",
       "min          1.000000\n",
       "25%         10.000000\n",
       "50%         13.000000\n",
       "75%         23.000000\n",
       "max       5634.000000\n",
       "Name: without_stopwords, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts_length = X_df['without_stopwords'].apply(lambda x: len(x.split()))\n",
    "# Now, let's analyze the distribution of these sequence lengths\n",
    "all_texts_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.85, min_df=0.01, stop_words='english')\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Get feature names to later identify words by index\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate the average TF-IDF score for each word across all documents\n",
    "avg_scores = np.mean(X.toarray(), axis=0)\n",
    "\n",
    "# Set a threshold, for example, the mean of average scores\n",
    "threshold = np.mean(avg_scores)\n",
    "\n",
    "# Filter words that have a score above the threshold\n",
    "important_words = [feature_names[i] for i in range(len(feature_names)) if avg_scores[i] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter corpus to keep only important words\n",
    "filtered_texts = []\n",
    "for doc in X_1:\n",
    "    tokens = doc.split()\n",
    "    filtered_tokens = [token for token in tokens if token in important_words]\n",
    "    filtered_texts.append(' '.join(filtered_tokens))\n",
    "\n",
    "# Tokenize texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(filtered_texts)\n",
    "sequences = tokenizer.texts_to_sequences(filtered_texts)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_seq_length = max(len(seq) for seq in sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 84 unique tokens.\n",
      "\n",
      "First 10 is listen below:\n",
      "{'nan': 1, 'people': 2, 'like': 3, 'say': 4, 'make': 5, 'just': 6, 'work': 7, 'right': 8, 'trump': 9, 'think': 10}\n"
     ]
    }
   ],
   "source": [
    "#Creating a word index of the words from the tokenizer \n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.\\n\\nFirst 10 is listen below:')\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters for the networks\n",
    "max_len = 100\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\"\n",
    "vocab_size = len(word_index)\n",
    "# This is fixed.\n",
    "embedding_dim = 100\n",
    "EPOCHS=20\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 1\n",
    "\n",
    "# Padding the sequences to keep the lengths uniform\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "# print('Shape of data tensor:', X_tensorflow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, max_len, num_classes, embeddings_GloVe):\n",
    "        self.max_len = max_len\n",
    "        self.num_classes = num_classes\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "\n",
    "\n",
    "# Create a configuration object\n",
    "config = ModelConfig(max_len=max_len, num_classes=num_classes, embeddings_GloVe=embeddings_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_LSTM_sequential(params, config):\n",
    "    input_layer = Input(shape=(config.max_len,), dtype='int32')\n",
    "    \n",
    "    # Use config object for fixed parameters such as embeddings\n",
    "    embedding = Embedding(input_dim=config.embeddings_GloVe.shape[0],\n",
    "                          output_dim=config.embeddings_GloVe.shape[1],\n",
    "                          weights=[config.embeddings_GloVe],\n",
    "                          trainable=False)(input_layer)\n",
    "    \n",
    "    # Use params dictionary for dynamic hyperparameters\n",
    "    dropout = Dropout(params['dropout_rate'])(embedding)\n",
    "\n",
    "    conv = Conv1D(filters=params['conv_filters'], kernel_size=1, activation='relu')(dropout)\n",
    "    conv = BatchNormalization()(conv)\n",
    "\n",
    "    lstm = Bidirectional(LSTM(params['lstm_units'], return_sequences=True, dropout=0.006, recurrent_dropout=0.1))(conv)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    \n",
    "    num_heads = 8\n",
    "    attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=config.embeddings_GloVe.shape[1] // num_heads, dropout=0.1)\n",
    "    attention_output = attention_layer(query=lstm, key=lstm, value=lstm)\n",
    "    attention_output = LayerNormalization()(attention_output)\n",
    "\n",
    "    dense = Dense(params['dense_2_units'], activation='relu')(attention_output)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    output = Dense(config.num_classes, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(params['learning_rate']), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import optuna\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from optuna.integration import KerasPruningCallback\n",
    "import optuna_dashboard\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-06 21:49:03,842] Using an existing study with name 'Proto_koer' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-05-06 21:49:17,151] Trial 65 failed with parameters: {'lstm_units': 135, 'dense_2_units': 156, 'dropout_rate': 0.3321002596160107, 'learning_rate': 0.0007439686229804321, 'conv_filters': 58} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\bugat\\AppData\\Local\\Temp\\ipykernel_13796\\519499413.py\", line 25, in objective\n",
      "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n",
      "    tmp_logs = self.train_function(iterator)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 832, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 888, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 695, in _initialize\n",
      "    self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py\", line 178, in trace_function\n",
      "    concrete_function = _maybe_define_function(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py\", line 283, in _maybe_define_function\n",
      "    concrete_function = _create_concrete_function(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py\", line 340, in _create_concrete_function\n",
      "    concrete_function = concrete_function_lib.ConcreteFunction.from_func_graph(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py\", line 1079, in from_func_graph\n",
      "    return ConcreteFunction(atomic_fn, shared_func_graph=shared_func_graph)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py\", line 1066, in __init__\n",
      "    self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py\", line 131, in __init__\n",
      "    self._attrs = atomic_fn.attributes\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 149, in attributes\n",
      "    attrs = self.definition.attr\n",
      "            ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 144, in definition\n",
      "    return self._bound_context.get_function_def(self.name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 1398, in get_function_def\n",
      "    function_def.ParseFromString(proto_data)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\message.py\", line 202, in ParseFromString\n",
      "    return self.MergeFromString(serialized)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1128, in MergeFromString\n",
      "    if self._InternalParse(serialized, 0, length) != length:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1195, in InternalParse\n",
      "    pos = field_decoder(buffer, new_pos, end, self, field_dict)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\decoder.py\", line 705, in DecodeRepeatedField\n",
      "    if value.add()._InternalParse(buffer, pos, new_pos) != new_pos:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 1195, in InternalParse\n",
      "    pos = field_decoder(buffer, new_pos, end, self, field_dict)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\decoder.py\", line 866, in DecodeMap\n",
      "    value[submsg.key].CopyFrom(submsg.value)\n",
      "                               ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 746, in getter\n",
      "    def getter(self):\n",
      "    \n",
      "KeyboardInterrupt\n",
      "[W 2024-05-06 21:49:17,179] Trial 65 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 52\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\u001b[39;00m\n\u001b[0;32m     45\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     46\u001b[0m                             sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(), \n\u001b[0;32m     47\u001b[0m                             study_name\u001b[38;5;241m=\u001b[39mstudy_name, \n\u001b[0;32m     48\u001b[0m                             storage\u001b[38;5;241m=\u001b[39mstorage_url, \n\u001b[0;32m     49\u001b[0m                             pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(),  \u001b[38;5;66;03m# Adding a pruner\u001b[39;00m\n\u001b[0;32m     50\u001b[0m                             load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 52\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[0;32m     63\u001b[0m             study,\n\u001b[0;32m     64\u001b[0m             func,\n\u001b[0;32m     65\u001b[0m             n_trials,\n\u001b[0;32m     66\u001b[0m             timeout,\n\u001b[0;32m     67\u001b[0m             catch,\n\u001b[0;32m     68\u001b[0m             callbacks,\n\u001b[0;32m     69\u001b[0m             gc_after_trial,\n\u001b[0;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     73\u001b[0m         )\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[57], line 25\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     23\u001b[0m pruning_callback \u001b[38;5;241m=\u001b[39m KerasPruningCallback(trial, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Create a pruning callback\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     26\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, pruning_callback], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     29\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:888\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 888\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(args, kwds, add_initializers_to\u001b[38;5;241m=\u001b[39minitializers)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    891\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    892\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:695\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    691\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    692\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    693\u001b[0m )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[0;32m    696\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    697\u001b[0m )\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    700\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[0;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[0;32m    180\u001b[0m   )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[0;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[0;32m    285\u001b[0m )\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:340\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    331\u001b[0m output_type \u001b[38;5;241m=\u001b[39m trace_type\u001b[38;5;241m.\u001b[39mfrom_value(\n\u001b[0;32m    332\u001b[0m     traced_func_graph\u001b[38;5;241m.\u001b[39mstructured_outputs, type_context\n\u001b[0;32m    333\u001b[0m )\n\u001b[0;32m    334\u001b[0m traced_func_type \u001b[38;5;241m=\u001b[39m function_type_lib\u001b[38;5;241m.\u001b[39mFunctionType(\n\u001b[0;32m    335\u001b[0m     function_type\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    336\u001b[0m     traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\u001b[38;5;241m.\u001b[39mcapture_types,\n\u001b[0;32m    337\u001b[0m     return_annotation\u001b[38;5;241m=\u001b[39moutput_type,\n\u001b[0;32m    338\u001b[0m )\n\u001b[1;32m--> 340\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m concrete_function_lib\u001b[38;5;241m.\u001b[39mConcreteFunction\u001b[38;5;241m.\u001b[39mfrom_func_graph(\n\u001b[0;32m    341\u001b[0m     traced_func_graph,\n\u001b[0;32m    342\u001b[0m     traced_func_type,\n\u001b[0;32m    343\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mattributes,\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m )\n\u001b[0;32m    350\u001b[0m _set_arg_keywords(concrete_function)\n\u001b[0;32m    351\u001b[0m transform\u001b[38;5;241m.\u001b[39mcall_concrete_function_callbacks(concrete_function)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1079\u001b[0m, in \u001b[0;36mConcreteFunction.from_func_graph\u001b[1;34m(cls, graph, function_type, attrs, shared_func_graph)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_func_graph\u001b[39m(\u001b[38;5;28mcls\u001b[39m, graph, function_type, attrs, shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1076\u001b[0m   atomic_fn \u001b[38;5;241m=\u001b[39m atomic_function\u001b[38;5;241m.\u001b[39mfrom_func_graph(\n\u001b[0;32m   1077\u001b[0m       _inference_name(graph\u001b[38;5;241m.\u001b[39mname), graph, attrs, function_type\n\u001b[0;32m   1078\u001b[0m   )\n\u001b[1;32m-> 1079\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ConcreteFunction(atomic_fn, shared_func_graph\u001b[38;5;241m=\u001b[39mshared_func_graph)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1066\u001b[0m, in \u001b[0;36mConcreteFunction.__init__\u001b[1;34m(self, atomic_fn, shared_func_graph)\u001b[0m\n\u001b[0;32m   1058\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_garbage_collector \u001b[38;5;241m=\u001b[39m ConcreteFunctionGarbageCollector(\n\u001b[0;32m   1059\u001b[0m       atomic_fn\u001b[38;5;241m.\u001b[39mgraph\n\u001b[0;32m   1060\u001b[0m   )\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# Pairs of forward and backward functions used for computing gradients.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# These each get a reference to the FuncGraph deleter since they use the\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;66;03m# FuncGraph directly.\u001b[39;00m\n\u001b[1;32m-> 1066\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_delayed_rewrite_functions \u001b[38;5;241m=\u001b[39m _DelayedRewriteGradientFunctions(\n\u001b[0;32m   1067\u001b[0m     atomic_fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_garbage_collector)\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_order_tape_functions \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_higher_order_tape_functions \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:131\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions.__init__\u001b[1;34m(self, atomic_fn, func_graph_deleter)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func_graph \u001b[38;5;241m=\u001b[39m atomic_fn\u001b[38;5;241m.\u001b[39mgraph\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function \u001b[38;5;241m=\u001b[39m atomic_fn\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs \u001b[38;5;241m=\u001b[39m atomic_fn\u001b[38;5;241m.\u001b[39mattributes\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Note that the FuncGraph is mutated later, so we need to inspect it now to\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# figure out the user-specified outputs of the inference function.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:149\u001b[0m, in \u001b[0;36mAtomicFunction.attributes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattributes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    148\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns FunctionDef attributes in the Runtime.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m   attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefinition\u001b[38;5;241m.\u001b[39mattr\n\u001b[0;32m    150\u001b[0m   \u001b[38;5;66;03m# Remove construction context since it is specific to runtime and this fn.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m   attrs\u001b[38;5;241m.\u001b[39mpop(attributes_lib\u001b[38;5;241m.\u001b[39mEAGER_RUNTIME_CONSTRUCTION_CONTEXT, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:144\u001b[0m, in \u001b[0;36mAtomicFunction.definition\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefinition\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m function_pb2\u001b[38;5;241m.\u001b[39mFunctionDef:\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Current FunctionDef in the Runtime.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mget_function_def(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1398\u001b[0m, in \u001b[0;36mContext.get_function_def\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1396\u001b[0m   proto_data \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(buffer_)\n\u001b[0;32m   1397\u001b[0m function_def \u001b[38;5;241m=\u001b[39m function_pb2\u001b[38;5;241m.\u001b[39mFunctionDef()\n\u001b[1;32m-> 1398\u001b[0m function_def\u001b[38;5;241m.\u001b[39mParseFromString(proto_data)\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function_def\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\message.py:202\u001b[0m, in \u001b[0;36mMessage.ParseFromString\u001b[1;34m(self, serialized)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse serialized protocol buffer data into this message.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03mLike :func:`MergeFromString()`, except we clear the object first.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m  message.DecodeError if the input cannot be parsed.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mClear()\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMergeFromString(serialized)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:1128\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.MergeFromString\u001b[1;34m(self, serialized)\u001b[0m\n\u001b[0;32m   1126\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(serialized)\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1128\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_InternalParse(serialized, \u001b[38;5;241m0\u001b[39m, length) \u001b[38;5;241m!=\u001b[39m length:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;66;03m# The only reason _InternalParse would return early is if it\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# encountered an end-group tag.\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m message_mod\u001b[38;5;241m.\u001b[39mDecodeError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected end-group tag.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m   1133\u001b[0m   \u001b[38;5;66;03m# Now ord(buf[p:p+1]) == ord('') gets TypeError.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:1195\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.InternalParse\u001b[1;34m(self, buffer, pos, end)\u001b[0m\n\u001b[0;32m   1193\u001b[0m   pos \u001b[38;5;241m=\u001b[39m new_pos\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m   pos \u001b[38;5;241m=\u001b[39m field_decoder(buffer, new_pos, end, \u001b[38;5;28mself\u001b[39m, field_dict)\n\u001b[0;32m   1196\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m field_desc:\n\u001b[0;32m   1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_UpdateOneofState(field_desc)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\decoder.py:705\u001b[0m, in \u001b[0;36mMessageDecoder.<locals>.DecodeRepeatedField\u001b[1;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[0;32m    703\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _DecodeError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruncated message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    704\u001b[0m \u001b[38;5;66;03m# Read sub-message.\u001b[39;00m\n\u001b[1;32m--> 705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39madd()\u001b[38;5;241m.\u001b[39m_InternalParse(buffer, pos, new_pos) \u001b[38;5;241m!=\u001b[39m new_pos:\n\u001b[0;32m    706\u001b[0m   \u001b[38;5;66;03m# The only reason _InternalParse would return early is if it\u001b[39;00m\n\u001b[0;32m    707\u001b[0m   \u001b[38;5;66;03m# encountered an end-group tag.\u001b[39;00m\n\u001b[0;32m    708\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _DecodeError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected end-group tag.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# Predict that the next tag is another copy of the same repeated field.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:1195\u001b[0m, in \u001b[0;36m_AddMergeFromStringMethod.<locals>.InternalParse\u001b[1;34m(self, buffer, pos, end)\u001b[0m\n\u001b[0;32m   1193\u001b[0m   pos \u001b[38;5;241m=\u001b[39m new_pos\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m   pos \u001b[38;5;241m=\u001b[39m field_decoder(buffer, new_pos, end, \u001b[38;5;28mself\u001b[39m, field_dict)\n\u001b[0;32m   1196\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m field_desc:\n\u001b[0;32m   1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_UpdateOneofState(field_desc)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\decoder.py:866\u001b[0m, in \u001b[0;36mMapDecoder.<locals>.DecodeMap\u001b[1;34m(buffer, pos, end, message, field_dict)\u001b[0m\n\u001b[0;32m    863\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m _DecodeError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected end-group tag.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_message_map:\n\u001b[1;32m--> 866\u001b[0m   value[submsg\u001b[38;5;241m.\u001b[39mkey]\u001b[38;5;241m.\u001b[39mCopyFrom(submsg\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    868\u001b[0m   value[submsg\u001b[38;5;241m.\u001b[39mkey] \u001b[38;5;241m=\u001b[39m submsg\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\python_message.py:746\u001b[0m, in \u001b[0;36m_AddPropertiesForNonRepeatedCompositeField.<locals>.getter\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    743\u001b[0m proto_field_name \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    744\u001b[0m property_name \u001b[38;5;241m=\u001b[39m _PropertyName(proto_field_name)\n\u001b[1;32m--> 746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetter\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    747\u001b[0m   field_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fields\u001b[38;5;241m.\u001b[39mget(field)\n\u001b[0;32m    748\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m field_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;66;03m# Construct a new object to represent this field.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the objective function including the logging of each trial's outcome\n",
    "def objective(trial):\n",
    "    # wandb.init(project=\"optuna_with_wandb\", entity=\"your_entity_here\", reinit=True)\n",
    "    params = {\n",
    "        'lstm_units': trial.suggest_int('lstm_units', 135, 145, step=1),\n",
    "        # 'dense_1_units': trial.suggest_int('dense_1_units', 100, 150, step=5),\n",
    "        'dense_2_units': trial.suggest_int('dense_2_units', 140, 160, step=2),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.3, 0.5),\n",
    "        # 'lstm_dropout': trial.suggest_float('lstm_dropout', 0.0, 0.2),\n",
    "        # 'lstm_recurrent': trial.suggest_float('lstm_recurrent', 0.0, 0.2),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "        'conv_filters': trial.suggest_int('conv_filters', 46, 60, step=2)\n",
    "    }\n",
    "\n",
    "    # # Log hyperparameters to wandb\n",
    "    # wandb.config.update(params)\n",
    "\n",
    "    # Build and train the model\n",
    "    model = CNN_LSTM_sequential(params, config)\n",
    "\n",
    "    #Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    pruning_callback = KerasPruningCallback(trial, 'val_loss')  # Create a pruning callback\n",
    "    # Fit the model\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10,\n",
    "                        callbacks=[early_stopping, pruning_callback], batch_size=32, verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
    "    \n",
    "    # Log final metrics to wandb\n",
    "    # wandb.log({\"loss\": loss, \"accuracy\": accuracy})\n",
    "\n",
    "    # Ensure wandb run is finished after each trial\n",
    "    # wandb.finish()\n",
    "    \n",
    "    # trial.report(accuracy, 1)  # Report the accuracy to Optuna\n",
    "    return accuracy \n",
    "\n",
    "\n",
    "# Setup Optuna with persistent storage\n",
    "storage_url = \"sqlite:///db.sqlite3\"\n",
    "study_name = 'Proto_koer'\n",
    "# optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study = optuna.create_study(direction='maximize', \n",
    "                            sampler=optuna.samplers.TPESampler(), \n",
    "                            study_name=study_name, \n",
    "                            storage=storage_url, \n",
    "                            pruner=optuna.pruners.MedianPruner(),  # Adding a pruner\n",
    "                            load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(f\"Best value: {study.best_value} (params: {study.best_params})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data to be saved as JSON\n",
    "# data = {\n",
    "#     \"params\": {\n",
    "#         \"lstm_units\": 135,\n",
    "#         \"dense_2_units\": 160,\n",
    "#         \"dropout_rate\": 0.43832284397692234,\n",
    "#         \"learning_rate\": 0.0004893640558066397,\n",
    "#         \"conv_filters\": 56\n",
    "#     },\n",
    "#     \"best_accuracy\": 0.73934644460678\n",
    "# }\n",
    "\n",
    "# # File path for JSON file\n",
    "# file_path = 'best_trial_tf_idf.json'\n",
    "\n",
    "# # Saving data as JSON\n",
    "# with open(file_path, 'w') as file:\n",
    "#     json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the params from CNN-LSTM from the saved JSON-fil\n",
    "with open(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_Notebooks\\best_trial_tf_idf.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    params = data['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "282/282 [==============================] - 201s 633ms/step - loss: 0.6517 - accuracy: 0.6424 - val_loss: 0.8923 - val_accuracy: 0.6477\n",
      "Epoch 2/30\n",
      "282/282 [==============================] - 175s 620ms/step - loss: 0.6251 - accuracy: 0.6636 - val_loss: 0.6043 - val_accuracy: 0.6856\n",
      "Epoch 3/30\n",
      "282/282 [==============================] - 174s 616ms/step - loss: 0.6215 - accuracy: 0.6710 - val_loss: 0.7118 - val_accuracy: 0.6825\n",
      "Epoch 4/30\n",
      "224/282 [======================>.......] - ETA: 2:33 - loss: 0.6116 - accuracy: 0.6822"
     ]
    }
   ],
   "source": [
    "best_model = CNN_LSTM_sequential(params, config)\n",
    "\n",
    "best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32, verbose=1)\n",
    "# Retrain on the full training data\n",
    "\n",
    "# Evaluate on the test data\n",
    "loss_1, accuracy_1 = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {accuracy_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_acc}, Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.title('Accuracy score')\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.show()\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.title('Loss value')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
