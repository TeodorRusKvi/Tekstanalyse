{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import optuna\n",
    "# # import wandb\n",
    "# import logging\n",
    "# import sys\n",
    "# import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, Bidirectional, MaxPooling1D, GlobalAveragePooling1D, AdditiveAttention, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import AdditiveAttention, Concatenate, BatchNormalization, Activation, MultiHeadAttention, LayerNormalization, TextVectorization, Masking, Reshape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'new_df.csv')\n",
    "\n",
    "X_df['y_liberal'] = y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12804"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming your DataFrame is named 'df' and the column you want to check is named 'column_name'\n",
    "empty_rows = X_df['without_stopwords'].isnull()  # Check for empty values in the column\n",
    "\n",
    "X_df = X_df.dropna(subset=['without_stopwords'])\n",
    "\n",
    "len(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    12804.000000\n",
      "mean       171.845205\n",
      "std        651.529329\n",
      "min          4.000000\n",
      "25%         42.000000\n",
      "50%         60.000000\n",
      "75%         93.000000\n",
      "max      17928.000000\n",
      "Name: length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Use lambda function to find the length of each row in the column\n",
    "X_df['length'] = X_df['without_stopwords'].apply(lambda x: len(x))\n",
    "\n",
    "# Print the DataFrame with the added 'length' column\n",
    "print(X_df['length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22340 unique tokens.\n",
      "\n",
      "First 10 is listen below:\n",
      "{'<OOV>': 1, 'nan': 2, 'people': 3, 'usa': 4, 'like': 5, 'work': 6, 'right': 7, 'trump': 8, 'think': 9, 'state': 10}\n"
     ]
    }
   ],
   "source": [
    "# Konverter kolonnen til et NumPy array\n",
    "X = X_df['without_stopwords'].astype(str).to_numpy().flatten()\n",
    "y = X_df['y_liberal'].to_numpy()\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>') # Hyperparameters = num_words=vocab_size, oov_token=oov_tok\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "#Creating a word index of the words from the tokenizer \n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.\\n\\nFirst 10 is listen below:')\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters for the networks\n",
    "max_len = 150\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\"\n",
    "vocab_size = len(word_index)\n",
    "embedding_dim = 100\n",
    "EPOCHS=20\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list = np.array([[element] for element in y])\n",
    "y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the sequences from the reddit posts\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "# Padding the sequences to keep the lengths uniform\n",
    "X = pad_sequences(X, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "# print('Shape of data tensor:', X_tensorflow.shape)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_list, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, max_len, num_classes, embeddings_GloVe):\n",
    "        self.max_len = max_len\n",
    "        self.num_classes = num_classes\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "\n",
    "\n",
    "# Create a configuration object\n",
    "config = ModelConfig(max_len=max_len, num_classes=num_classes, embeddings_GloVe=embeddings_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_LSTM_sequential(params, config):\n",
    "    input_layer = Input(shape=(config.max_len,), dtype='int32')\n",
    "    \n",
    "    # Use config object for fixed parameters such as embeddings\n",
    "    embedding = Embedding(input_dim=config.embeddings_GloVe.shape[0],\n",
    "                          output_dim=config.embeddings_GloVe.shape[1],\n",
    "                          weights=[config.embeddings_GloVe],\n",
    "                          trainable=False)(input_layer)\n",
    "    \n",
    "    # Use params dictionary for dynamic hyperparameters\n",
    "    dropout = Dropout(params['dropout_rate'])(embedding)\n",
    "\n",
    "    conv = Conv1D(filters=params['conv_filters'], kernel_size=1, activation='relu')(dropout)\n",
    "    conv = BatchNormalization()(conv)\n",
    "\n",
    "    lstm = Bidirectional(LSTM(params['lstm_units'], return_sequences=True, dropout=0.006, recurrent_dropout=0.1))(conv)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "    \n",
    "    num_heads = 8\n",
    "    attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=config.embeddings_GloVe.shape[1] // num_heads, dropout=0.1)\n",
    "    attention_output = attention_layer(query=lstm, key=lstm, value=lstm)\n",
    "    attention_output = LayerNormalization()(attention_output)\n",
    "\n",
    "    dense = Dense(params['dense_2_units'], activation='relu')(attention_output)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    output = Dense(config.num_classes, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(params['learning_rate']), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv1D, MaxPooling1D, Flatten, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def simple_CNN_model(params, config):\n",
    "    \n",
    "    # Define input\n",
    "    input_text = Input(shape=(max_len,), dtype=tf.int32)\n",
    "    \n",
    "    # Embedding Layer\n",
    "    embedding_layer = Embedding(input_dim=config.embeddings_GloVe.shape[0],\n",
    "                                output_dim=config.embeddings_GloVe.shape[1],\n",
    "                                weights=[embeddings_GloVe],\n",
    "                                trainable=False)\n",
    "    \n",
    "    embedded_seq = embedding_layer(input_text)\n",
    "\n",
    "    # # Masking Layer\n",
    "    # masked_input = Masking(mask_value=0.0)(embedded_seq)\n",
    "    \n",
    "    # # CNN Branch\n",
    "    # reshaped_input = Reshape((max_len, embeddings_GloVe.shape[1]))(masked_input)\n",
    "    \n",
    "    # CNN Branch\n",
    "    # reshaped_input = Reshape((max_len, embeddings_GloVe.shape[1]))(embedded_seq)\n",
    "    dropout_cnn = Dropout(params['dropout_rate'])(embedded_seq)\n",
    "    batch = BatchNormalization()(dropout_cnn)\n",
    "    cnn = Conv1D(params['conv_filters'], 3, activation='relu')(batch)\n",
    "    dropout_cnn = Dropout(0.2)(cnn)\n",
    "    cnn = BatchNormalization()(dropout_cnn)\n",
    "    cnn = Flatten()(cnn)\n",
    "\n",
    "    # Dense Layer\n",
    "    dense = Dense(params['dense_2_units'], activation='relu')(cnn)\n",
    "    dropout_final = Dropout(0.2)(dense)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(num_classes, activation='sigmoid')(dropout_final)  # Adjust the number of classes as needed\n",
    "\n",
    "    # Define optimizer with the specified learning rate\n",
    "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    # Build the model\n",
    "    model = Model(inputs=input_text, outputs=output)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\bugat\\GIT\\Tekstanalyse\\git_NLP_Notebooks\\best_trial_length_100.json', 'r') as f:\n",
    "    trial = json.load(f)\n",
    "    params = trial['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lstm_units': 140,\n",
       " 'dense_2_units': 150,\n",
       " 'dropout_rate': 0.42803898610506674,\n",
       " 'learning_rate': 0.000922823163674921,\n",
       " 'conv_filters': 52}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_LSTM_sequential(params, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'numpy.int64\\'>\"})'})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compile and train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train,y_train, \n\u001b[0;32m      3\u001b[0m           validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val), \n\u001b[0;32m      4\u001b[0m           epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      5\u001b[0m           batch_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, \n\u001b[0;32m      6\u001b[0m           verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1105\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1102\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1107\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1108\u001b[0m         )\n\u001b[0;32m   1109\u001b[0m     )\n\u001b[0;32m   1110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1115\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'numpy.int64\\'>\"})'})"
     ]
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.fit(X_train,y_train, \n",
    "          validation_data=(X_val, y_val), \n",
    "          epochs=20,\n",
    "          batch_size= 32, \n",
    "          verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
