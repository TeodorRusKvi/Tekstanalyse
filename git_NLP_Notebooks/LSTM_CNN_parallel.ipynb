{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "# import wandb\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, Bidirectional, MaxPooling1D, GlobalAveragePooling1D, AdditiveAttention, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import AdditiveAttention, Concatenate, BatchNormalization, Activation, MultiHeadAttention, LayerNormalization, TextVectorization, Masking\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_data.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y = y_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'new_df.csv')\n",
    "\n",
    "X_1= X_df['All_text']\n",
    "\n",
    "# Konverter kolonnen til et NumPy array\n",
    "X = X_df['All_text'].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34985 unique tokens.\n",
      "\n",
      "First 10 is listen below:\n",
      "{'<OOV>': 1, 'the': 2, 'to': 3, 'of': 4, 'and': 5, 'a': 6, 'in': 7, '0': 8, 'is': 9, 'that': 10}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>') # Hyperparameters = num_words=vocab_size, oov_token=oov_tok\n",
    "tokenizer.fit_on_texts(X_1)\n",
    "\n",
    "#Creating a word index of the words from the tokenizer \n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.\\n\\nFirst 10 is listen below:')\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters for the networks\n",
    "max_len = 100\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\"\n",
    "vocab_size = len(word_index)\n",
    "# This is fixed.\n",
    "embedding_dim = 100\n",
    "EPOCHS=20\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf.data.Dataset from texts and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_1, y))\n",
    "\n",
    "# Shuffle the dataset (if needed)\n",
    "dataset = dataset.shuffle(buffer_size=len(X_1), reshuffle_each_iteration=False)\n",
    "\n",
    "# Determine split sizes\n",
    "train_size = int(0.7 * len(X_1))\n",
    "val_size = int(0.15 * len(X_1))\n",
    "test_size = len(X_1) - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(test_size)\n",
    "test_dataset = test_dataset.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "int_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len\n",
    ")\n",
    "\n",
    "hot_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='multi_hot',\n",
    ")\n",
    "\n",
    "# Prepare the data for adaptation\n",
    "all_texts = dataset.map(lambda x, y: x)\n",
    "\n",
    "int_vectorization.adapt(all_texts)\n",
    "hot_vectorization.adapt(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label, vectorize_layer):\n",
    "    text = vectorize_layer(text)\n",
    "    return text, label\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: vectorize_text(x, y, int_vectorization))\n",
    "val_dataset = val_dataset.map(lambda x, y: vectorize_text(x, y, hot_vectorization))\n",
    "test_dataset = test_dataset.map(lambda x, y: vectorize_text(x, y, int_vectorization))  # or whichever is appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, max_len, num_classes, embeddings_GloVe):\n",
    "        self.max_len = max_len\n",
    "        self.num_classes = num_classes\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "\n",
    "\n",
    "# Create a configuration object\n",
    "config = ModelConfig(max_len=max_len, num_classes=num_classes, embeddings_GloVe=embeddings_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv1D, MaxPooling1D, Flatten, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def CNN_LSTM_parallel(params, config):\n",
    "    \n",
    "    # Define two separate inputs\n",
    "    input_text_for_lstm = Input(shape=(max_len,), dtype=tf.string)\n",
    "    input_text_for_cnn = Input(shape=(vocab_size,), dtype=tf.string)\n",
    "\n",
    "    # LSTM Branch\n",
    "    embedding_layer = Embedding(input_dim=embeddings_GloVe.shape[0],\n",
    "                                output_dim=embeddings_GloVe.shape[1],\n",
    "                                weights=[embeddings_GloVe],\n",
    "                                trainable=False)\n",
    "    # masked_input = Masking(mask_value=0)(input_text_for_lstm)  # Masking layer added to ignore zeros (padding)\n",
    "    embedded_seq = embedding_layer(input_text_for_lstm)\n",
    "    dropout = Dropout(params['dropout_rate'])(embedded_seq)\n",
    "    lstm = LSTM(params['lstm_units'])(dropout)\n",
    "    lstm = LayerNormalization()(lstm)\n",
    "\n",
    "    # CNN Branch\n",
    "    cnn = Conv1D(params['conv_filters'], 1, activation='relu')(input_text_for_cnn)\n",
    "    dense = BatchNormalization()(cnn)\n",
    "    cnn = MaxPooling1D(2)(dense)\n",
    "    cnn = Flatten()(cnn)\n",
    "\n",
    "    # Concatenate\n",
    "    concatenated = concatenate([lstm, cnn])\n",
    "\n",
    "\n",
    "    # num_heads = 8 # This should divide embed_dim evenly\n",
    "    # attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embeddings_GloVe.shape[1] // num_heads, dropout=0.0)\n",
    "    # attention_output = attention_layer(query=lstm, key=lstm, value=lstm)\n",
    "    # attention_output= LayerNormalization()(attention_output)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(2, activation='softmax')(concatenated)  # Assuming 10 classes\n",
    "\n",
    "    # Build the model\n",
    "    model = Model(inputs=[input_text_for_lstm, input_text_for_cnn], outputs=output)\n",
    "\n",
    "    model.compile(optimizer=Adam(params['learning_rate']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_trial_length_100.json', 'r') as f:\n",
    "    trial = json.load(f)\n",
    "    params = trial['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv1d_2\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 34986)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m CNN_LSTM_parallel(params, config)\n",
      "Cell \u001b[1;32mIn[31], line 22\u001b[0m, in \u001b[0;36mCNN_LSTM_parallel\u001b[1;34m(params, config)\u001b[0m\n\u001b[0;32m     19\u001b[0m lstm \u001b[38;5;241m=\u001b[39m LayerNormalization()(lstm)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# CNN Branch\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m cnn \u001b[38;5;241m=\u001b[39m Conv1D(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv_filters\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(input_text_for_cnn)\n\u001b[0;32m     23\u001b[0m dense \u001b[38;5;241m=\u001b[39m BatchNormalization()(cnn)\n\u001b[0;32m     24\u001b[0m cnn \u001b[38;5;241m=\u001b[39m MaxPooling1D(\u001b[38;5;241m2\u001b[39m)(dense)\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py:253\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    251\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    254\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m         )\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"conv1d_2\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 34986)"
     ]
    }
   ],
   "source": [
    "model = CNN_LSTM_parallel(params, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=20, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
