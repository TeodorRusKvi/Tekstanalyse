{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# # import wandb\n",
    "# import logging\n",
    "# import sys\n",
    "# import os\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, Bidirectional, MaxPooling1D, GlobalAveragePooling1D, AdditiveAttention, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import AdditiveAttention, Concatenate, BatchNormalization, Activation, MultiHeadAttention, LayerNormalization, TextVectorization, Masking, Reshape\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_data.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y = y_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'new_df.csv')\n",
    "\n",
    "X_1= X_df['All_text']\n",
    "\n",
    "# Konverter kolonnen til et NumPy array\n",
    "X = X_df['All_text'].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34985 unique tokens.\n",
      "\n",
      "First 10 is listen below:\n",
      "{'<OOV>': 1, 'the': 2, 'to': 3, 'of': 4, 'and': 5, 'a': 6, 'in': 7, '0': 8, 'is': 9, 'that': 10}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>') # Hyperparameters = num_words=vocab_size, oov_token=oov_tok\n",
    "tokenizer.fit_on_texts(X_1)\n",
    "\n",
    "#Creating a word index of the words from the tokenizer \n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.\\n\\nFirst 10 is listen below:')\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing hyperparameters for the networks\n",
    "max_len = 100\n",
    "trunc_type = \"post\"\n",
    "padding_type = \"post\"\n",
    "vocab_size = len(word_index)\n",
    "# This is fixed.\n",
    "embedding_dim = 100\n",
    "EPOCHS=20\n",
    "BATCH_SIZE = 32\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf.data.Dataset from texts and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_1, y))\n",
    "\n",
    "# Shuffle the dataset (if needed)\n",
    "dataset = dataset.shuffle(buffer_size=len(X_1), reshuffle_each_iteration=False)\n",
    "\n",
    "# Determine split sizes\n",
    "train_size = int(0.7 * len(X_1))\n",
    "val_size = int(0.15 * len(X_1))\n",
    "test_size = len(X_1) - train_size - val_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "val_dataset = test_dataset.skip(test_size)\n",
    "test_dataset = test_dataset.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len\n",
    ")\n",
    "\n",
    "hot_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='multi_hot',\n",
    ")\n",
    "\n",
    "# Prepare the data for adaptation\n",
    "all_texts = dataset.map(lambda x, y: x)\n",
    "\n",
    "int_vectorization.adapt(all_texts)\n",
    "hot_vectorization.adapt(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label, vectorize_layer):\n",
    "    text = vectorize_layer(text)\n",
    "    return text, label\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: vectorize_text(x, y, hot_vectorization))\n",
    "val_dataset = val_dataset.map(lambda x, y: vectorize_text(x, y, hot_vectorization))\n",
    "test_dataset = test_dataset.map(lambda x, y: vectorize_text(x, y, hot_vectorization))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, max_len, num_classes, embeddings_GloVe):\n",
    "        self.max_len = max_len\n",
    "        self.num_classes = num_classes\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "\n",
    "\n",
    "# Create a configuration object\n",
    "config = ModelConfig(max_len=max_len, num_classes=num_classes, embeddings_GloVe=embeddings_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv1D, MaxPooling1D, Flatten, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def CNN_LSTM_parallel(params, config):\n",
    "    \n",
    "    # Define two separate inputs\n",
    "    # input_text_for_lstm = Input(shape=(max_len,), dtype=tf.string)\n",
    "    # input_text = Input(shape=(vocab_size,), dtype=tf.string)\n",
    "\n",
    "    # # LSTM Branch\n",
    "    # embedding_layer = Embedding(input_dim=embeddings_GloVe.shape[0],\n",
    "    #                             output_dim=embeddings_GloVe.shape[1],\n",
    "    #                             weights=[embeddings_GloVe],\n",
    "    #                             trainable=False)\n",
    "    # # masked_input = Masking(mask_value=0)(input_text_for_lstm)  # Masking layer added to ignore zeros (padding)\n",
    "    # embedded_seq = embedding_layer(input_text_for_lstm)\n",
    "    # dropout = Dropout(params['dropout_rate'])(embedded_seq)\n",
    "    # lstm = LSTM(params['lstm_units'])(dropout)\n",
    "    # lstm = LayerNormalization()(lstm)\n",
    "\n",
    "    # CNN Branch\n",
    "    reshape_layer = Reshape((vocab_size, 1))  # Reshape to include the channel dimension\n",
    "    input_text_for_cnn = Input(shape=(vocab_size,))\n",
    "    reshaped_input = reshape_layer(input_text_for_cnn)\n",
    "\n",
    "\n",
    "    cnn = Conv1D(params['conv_filters'], 1, activation='relu')(reshaped_input)\n",
    "    cnn = BatchNormalization()(cnn)\n",
    "    # cnn = MaxPooling1D(2)(dense)\n",
    "    cnn = Flatten()(cnn)\n",
    "\n",
    "    # # Concatenate\n",
    "    # concatenated = concatenate([lstm, cnn])\n",
    "\n",
    "    # dense = Dense(params['dense_2_units'])(input_text)\n",
    "\n",
    "\n",
    "    # num_heads = 8 # This should divide embed_dim evenly\n",
    "    # attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embeddings_GloVe.shape[1] // num_heads, dropout=0.0)\n",
    "    # attention_output = attention_layer(query=dense, key=dense, value=dense)\n",
    "    # attention_output= LayerNormalization()(attention_output)\n",
    "\n",
    "    # Output layer\n",
    "    output = Dense(2, activation='softmax')(cnn)  # Assuming 10 classes\n",
    "\n",
    "    # Build the model\n",
    "    model = Model(inputs=[input_text_for_cnn], outputs=output)\n",
    "\n",
    "    model.compile(optimizer=Adam(params['learning_rate']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_trial_length_100.json', 'r') as f:\n",
    "    trial = json.load(f)\n",
    "    params = trial['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lstm_units': 140,\n",
       " 'dense_2_units': 150,\n",
       " 'dropout_rate': 0.42803898610506674,\n",
       " 'learning_rate': 0.000922823163674921,\n",
       " 'conv_filters': 52}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_LSTM_parallel(params, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "258/282 [==========================>...] - ETA: 20s - loss: 0.9064 - accuracy: 0.6923"
     ]
    }
   ],
   "source": [
    "# Compile and train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=20, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
