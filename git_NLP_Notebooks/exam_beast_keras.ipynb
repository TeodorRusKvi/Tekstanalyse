{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow==2.15.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, TimeDistributed, Bidirectional, Concatenate, GlobalAveragePooling1D, AdditiveAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'X_tensorflow.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "X = X_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y = y_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Conv1D, Dense, TimeDistributed, Bidirectional, LSTM, Concatenate, AdditiveAttention, GlobalAveragePooling1D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Definer modellen som en funksjon for bruk med KerasClassifier\n",
    "def build_model(conv_filter_units=32, dense_units=100, lstm_units=80, lstm_dropout_rate=0.02, lstm_r_dropout_rate=0.05, learning_rate=0.001, dropout_rate=0.05):\n",
    "    sequence_input = Input(shape=(input_length,), dtype='int32')\n",
    "    embedded_sequences = Embedding(input_dim=embeddings_GloVe.shape[0],\n",
    "                                   output_dim=embeddings_GloVe.shape[1],\n",
    "                                   weights=[embeddings_GloVe],\n",
    "                                   trainable=False)(sequence_input)\n",
    "\n",
    "    conv_blocks = []\n",
    "    lstm_blocks = []\n",
    "\n",
    "    for _ in range(parallel_blocks):\n",
    "        conv = Conv1D(\n",
    "            filters=conv_filter_units,\n",
    "            kernel_size=1,\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            strides=1)(embedded_sequences)\n",
    "        conv_dense = TimeDistributed(Dense(dense_units, activation='relu'))(conv)\n",
    "        conv_blocks.append(conv_dense)\n",
    "\n",
    "        lstm = Bidirectional(LSTM(\n",
    "            units=lstm_units,\n",
    "            return_sequences=True,\n",
    "            dropout=lstm_dropout_rate,\n",
    "            recurrent_dropout=lstm_r_dropout_rate\n",
    "        ))(conv_dense)\n",
    "        lstm_blocks.append(lstm)\n",
    "\n",
    "    combined = Concatenate()(conv_blocks + lstm_blocks)\n",
    "    attention_layer = AdditiveAttention(use_scale=True)\n",
    "    attention_output = attention_layer([combined, combined], return_attention_scores=False)\n",
    "    context_vector = GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "    dense = Dense(units=dense_units, activation='relu')(context_vector)\n",
    "    dropout = Dropout(dropout_rate)(dense)\n",
    "    outdata = Dense(num_classes, activation='sigmoid')(dropout)\n",
    "    model = Model(inputs=sequence_input, outputs=outdata)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_length = 20\n",
    "num_classes = 1\n",
    "parallel_blocks = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to Embedding: {'weights': [array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.29019001,  0.80497003,  0.31187001, ..., -0.33603001,\n         0.45998001, -0.11278   ],\n       ...,\n       [-0.26343   ,  0.34340999,  0.032067  , ..., -0.25034001,\n        -0.13135   ,  0.45581001],\n       [-0.39555001,  0.067909  , -0.063694  , ..., -0.37257999,\n         0.63815999,  0.44885999],\n       [ 0.22718   , -0.41553   , -0.15654001, ...,  0.30133   ,\n        -0.21064   , -0.58592999]])]}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m build_model()\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val))\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(conv_filter_units, dense_units, lstm_units, lstm_dropout_rate, lstm_r_dropout_rate, learning_rate, dropout_rate)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(conv_filter_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, dense_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, lstm_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, lstm_dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m, lstm_r_dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m):\n\u001b[0;32m     10\u001b[0m     sequence_input \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(input_length,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     embedded_sequences \u001b[38;5;241m=\u001b[39m Embedding(input_dim\u001b[38;5;241m=\u001b[39membeddings_GloVe\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     12\u001b[0m                                    output_dim\u001b[38;5;241m=\u001b[39membeddings_GloVe\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     13\u001b[0m                                    weights\u001b[38;5;241m=\u001b[39m[embeddings_GloVe],\n\u001b[0;32m     14\u001b[0m                                    trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)(sequence_input)\n\u001b[0;32m     16\u001b[0m     conv_blocks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m     lstm_blocks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:89\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `input_length` is deprecated. Just remove it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     88\u001b[0m     )\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
      "File \u001b[1;32mc:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:263\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m     )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;241m=\u001b[39m autocast\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'weights': [array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.29019001,  0.80497003,  0.31187001, ..., -0.33603001,\n         0.45998001, -0.11278   ],\n       ...,\n       [-0.26343   ,  0.34340999,  0.032067  , ..., -0.25034001,\n        -0.13135   ,  0.45581001],\n       [-0.39555001,  0.067909  , -0.063694  , ..., -0.37257999,\n         0.63815999,  0.44885999],\n       [ 0.22718   , -0.41553   , -0.15654001, ...,  0.30133   ,\n        -0.21064   , -0.58592999]])]}"
     ]
    }
   ],
   "source": [
    "model= build_model()\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 20, 100)              2223500   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 20, 114)              3648      ['conv1d[0][0]']              \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 20, 114)              3648      ['conv1d_1[0][0]']            \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 20, 216)              192672    ['time_distributed[0][0]']    \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 20, 216)              192672    ['time_distributed_1[0][0]']  \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 20, 660)              0         ['time_distributed[0][0]',    \n",
      "                                                                     'time_distributed_1[0][0]',  \n",
      "                                                                     'bidirectional[0][0]',       \n",
      "                                                                     'bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " additive_attention (Additi  (None, 20, 660)              660       ['concatenate[0][0]',         \n",
      " veAttention)                                                        'concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 660)                  0         ['additive_attention[0][0]']  \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 150)                  99150     ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 150)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1)                    151       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2722363 (10.38 MB)\n",
      "Trainable params: 498863 (1.90 MB)\n",
      "Non-trainable params: 2223500 (8.48 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
