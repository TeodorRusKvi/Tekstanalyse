{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.15.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, TimeDistributed, Bidirectional, Concatenate, GlobalAveragePooling1D, AdditiveAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'X_tensorflow.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "X = X_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y = y_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 74\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Initialize Weights & Biases\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# wandb.init(project=\"LSTM_CNN_Attention\")\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[0;32m     73\u001b[0m hypermodel \u001b[38;5;241m=\u001b[39m TextClassifierHyperModel(input_length, embeddings_GloVe, num_classes, parallel_blocks)\n\u001b[1;32m---> 74\u001b[0m model \u001b[38;5;241m=\u001b[39m hypermodel\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Model summary\u001b[39;00m\n\u001b[0;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[7], line 55\u001b[0m, in \u001b[0;36mTextClassifierHyperModel.build\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_layer([combined, combined], return_attention_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# context_vector = GlobalAveragePooling1D()(attention_output)\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m dense \u001b[38;5;241m=\u001b[39m Dense(units\u001b[38;5;241m=\u001b[39mdense_units)(context_vector)\n\u001b[0;32m     56\u001b[0m dropout \u001b[38;5;241m=\u001b[39m Dropout(dropout_rate)(dense)\n\u001b[0;32m     57\u001b[0m outdata \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)(dropout)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'context_vector' is not defined"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, TimeDistributed, Bidirectional, Concatenate, GlobalAveragePooling1D, AdditiveAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "class TextClassifierHyperModel:\n",
    "    def __init__(self, input_shape, embeddings_GloVe, num_classes, parallel_blocks):\n",
    "        self.input_shape = input_shape\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "        self.num_classes = num_classes\n",
    "        self.parallel_blocks = parallel_blocks\n",
    "\n",
    "    def build(self):\n",
    "        sequence_input = Input(shape=(self.input_shape,), dtype='int32')\n",
    "        embedded_sequences = Embedding(input_dim=self.embeddings_GloVe.shape[0],\n",
    "                                       output_dim=self.embeddings_GloVe.shape[1],\n",
    "                                       weights=[self.embeddings_GloVe],\n",
    "                                       trainable=False)(sequence_input)\n",
    "\n",
    "        conv_blocks = []\n",
    "        lstm_blocks = []\n",
    "\n",
    "        # Adjusted hyperparameters\n",
    "        conv_filter_units = 32\n",
    "        time_units = 96\n",
    "        lstm_units = 80\n",
    "        dense_units = 130\n",
    "        lstm_dropout_rate = 0.02  # Increased dropout rate\n",
    "        lstm_r_dropout_rate = 0.015\n",
    "        dropout_rate = 0.1\n",
    "        learning_rate = 0.002\n",
    "\n",
    "        for _ in range(self.parallel_blocks):\n",
    "            conv = Conv1D(filters=conv_filter_units, kernel_size=1, padding='same', strides=1)(embedded_sequences)\n",
    "            # conv_dense = TimeDistributed(Dense(time_units))(conv)\n",
    "            conv_dense = BatchNormalization()(conv)  # Batch Normalization after dense layer\n",
    "            conv_blocks.append(conv_dense)\n",
    "            lstm = Bidirectional(LSTM(units=lstm_units, return_sequences=True, dropout=lstm_dropout_rate, recurrent_dropout=lstm_r_dropout_rate))(conv_dense)\n",
    "            lstm_blocks.append(lstm)\n",
    "\n",
    "        combined = Concatenate()(conv_blocks + lstm_blocks)\n",
    "        attention_layer = AdditiveAttention(use_scale=True)\n",
    "        attention_output = attention_layer([combined, combined], return_attention_scores=False)\n",
    "        context_vector = GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "        dense = Dense(units=dense_units)(context_vector)\n",
    "        dropout = Dropout(dropout_rate)(dense)\n",
    "        outdata = Dense(self.num_classes, activation='sigmoid')(dropout)\n",
    "        model = Model(inputs=sequence_input, outputs=outdata)\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "input_length = 20\n",
    "num_classes = 1\n",
    "parallel_blocks = 2\n",
    "EPOCHS=20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "# wandb.init(project=\"LSTM_CNN_Attention\")\n",
    "\n",
    "# Instantiate the model\n",
    "hypermodel = TextClassifierHyperModel(input_length, embeddings_GloVe, num_classes, parallel_blocks)\n",
    "model = hypermodel.build()\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=early_stopping_callback)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 2s 30ms/step - loss: 0.7102 - accuracy: 0.7361\n",
      "Test Loss: 0.710237443447113\n",
      "Test Accuracy: 0.7361327409744263\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbugat\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProsjekter\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTekstanalyse\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgit_NLP\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTekstanalyse\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBeast_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "best_model.save(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
