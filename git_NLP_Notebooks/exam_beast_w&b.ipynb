{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.15.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, TimeDistributed, Bidirectional, Concatenate, GlobalAveragePooling1D, AdditiveAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22234 unique tokens.\n",
      "\n",
      "First 10 is listen below:\n",
      "{'<OOV>': 1, 'people': 2, 'like': 3, 'work': 4, 'right': 5, 'trump': 6, 'think': 7, 'state': 8, 'government': 9, 'party': 10}\n"
     ]
    }
   ],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "df = pd.read_csv(url_data + 'new_df.csv')\n",
    "\n",
    "df['All_text'] = df['All_text'].replace(['U.S.', 'U.S.A.'], ['US', 'USA'], regex=True)\n",
    "df['Processed'] = df['Processed'].fillna(0)\n",
    "df['Processed'] = df['Processed'].astype(str)\n",
    "df['All_text'] = df['All_text'].fillna(0)\n",
    "df['All_text'] = df['All_text'].astype(str)\n",
    "\n",
    "# df.to_csv('new_df.csv', index=False)\n",
    "\n",
    "# Making the relevant columns to lists\n",
    "all_texts = (df['All_text'].to_list())\n",
    "texts = df['Processed'].to_list()\n",
    "\n",
    "# Setting the wanted text for further modelling\n",
    "corpus = texts\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>') # Hyperparameters = num_words=vocab_size, oov_token=oov_tok\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.\\n\\nFirst 10 is listen below:')\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_data\\file_name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Political Lean\n",
       "Liberal         8319\n",
       "Conservative    4535\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_lean_counts = df['Political Lean'].value_counts()\n",
    "political_lean_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_train_LSTM = pd.read_csv(url_data+'X_tensorflow.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "X_train_LSTM = X_train_LSTM.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_train_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y_train_LSTM = y_train_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_LSTM, y_train_LSTM, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mteodor-ruskvi\u001b[0m (\u001b[33mteodor_ruskvi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae57c36c8ce4aa6acbeb195d11656b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777777609622, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_Notebooks\\wandb\\run-20240417_082356-jmw5775k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/teodor_ruskvi/Beast_model/runs/jmw5775k' target=\"_blank\">rare-eon-2</a></strong> to <a href='https://wandb.ai/teodor_ruskvi/Beast_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/teodor_ruskvi/Beast_model' target=\"_blank\">https://wandb.ai/teodor_ruskvi/Beast_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/teodor_ruskvi/Beast_model/runs/jmw5775k' target=\"_blank\">https://wandb.ai/teodor_ruskvi/Beast_model/runs/jmw5775k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. learning_rate uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: q08mwc1p\n",
      "Sweep URL: https://wandb.ai/teodor_ruskvi/text_classification/sweeps/q08mwc1p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Exception in thread ChkStopThrIntMsgThr:\n",
      ":\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zk78vq6k with config:\n",
      "    \u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filter_units: 30\n",
      "self.run()    \n",
      "self.run()\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\threading.py\", line 982, in run\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\threading.py\", line 982, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_lstm_units: 124\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_units: 143\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 300, in check_internal_messages\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 286, in check_stop_status\n",
      "    self._loop_check_status(\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 224, in _loop_check_status\n",
      "    local_handle = request()\n",
      "                   ^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 856, in deliver_internal_messages\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.07670160451461043\n",
      "    self._loop_check_status(\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 224, in _loop_check_status\n",
      "    return self._deliver_internal_messages(internal_message)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0023707830229623018\n",
      "^    ^local_handle = request()^^^^^^^^\n",
      "^\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_dropout_rate: 0.00595299982061829\n",
      "^ ^ ^ ^^^^^^^\n",
      "   File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 516, in _deliver_internal_messages\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_r_dropout_rate: 0.05546957764720482\n",
      "               ^^\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_units: 100\n",
      "^^^^    ^return self._deliver_record(record)\n",
      "    ^ ^ \n",
      "    File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 840, in deliver_stop_status\n",
      "   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 459, in _deliver_record\n",
      "    return self._deliver_stop_status(status)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 494, in _deliver_stop_status\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "    return self._deliver_record(record)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 459, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      "    interface._publish(record)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      "    self._send_message(msg)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "    self.send_server_request(server_req)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    self._send_message(msg)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "    sent = self._sock.send(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionAbortedError: [WinError 10053] En etablert tilkobling ble avbrutt av programvaren pÃ¥ vertsmaskinen\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionAbortedError: [WinError 10053] En etablert tilkobling ble avbrutt av programvaren pÃ¥ vertsmaskinen\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_Notebooks\\wandb\\run-20240417_082435-zk78vq6k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/teodor_ruskvi/text_classification/runs/zk78vq6k' target=\"_blank\">worldly-sweep-1</a></strong> to <a href='https://wandb.ai/teodor_ruskvi/text_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/teodor_ruskvi/text_classification/sweeps/q08mwc1p' target=\"_blank\">https://wandb.ai/teodor_ruskvi/text_classification/sweeps/q08mwc1p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/teodor_ruskvi/text_classification' target=\"_blank\">https://wandb.ai/teodor_ruskvi/text_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/teodor_ruskvi/text_classification/sweeps/q08mwc1p' target=\"_blank\">https://wandb.ai/teodor_ruskvi/text_classification/sweeps/q08mwc1p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/teodor_ruskvi/text_classification/runs/zk78vq6k' target=\"_blank\">https://wandb.ai/teodor_ruskvi/text_classification/runs/zk78vq6k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TextClassifier_WB:\n",
    "    def __init__(self, input_shape, embeddings_GloVe, num_classes, parallel_blocks):\n",
    "        self.input_shape = input_shape\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "        self.num_classes = num_classes\n",
    "        self.parallel_blocks = parallel_blocks\n",
    "\n",
    "    def build(self):\n",
    "        config = wandb.config\n",
    "        sequence_input = Input(shape=(self.input_shape,), dtype='int32')\n",
    "        embedded_sequences = Embedding(input_dim=self.embeddings_GloVe.shape[0],\n",
    "                                       output_dim=self.embeddings_GloVe.shape[1],\n",
    "                                       weights=[self.embeddings_GloVe],\n",
    "                                       trainable=False)(sequence_input)\n",
    "\n",
    "        conv_blocks = []\n",
    "        lstm_blocks = []\n",
    "\n",
    "        for _ in range(self.parallel_blocks):\n",
    "            conv = Conv1D(\n",
    "                filters=config.conv_filter_units,\n",
    "                kernel_size=1,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                strides=1)(embedded_sequences)\n",
    "            conv_dense = TimeDistributed(Dense(config.dense_units, activation='relu'))(conv)\n",
    "            conv_blocks.append(conv_dense)\n",
    "\n",
    "            lstm = Bidirectional(LSTM(\n",
    "                units=config.lstm_units,\n",
    "                return_sequences=True,\n",
    "                dropout=config.lstm_dropout_rate,\n",
    "                recurrent_dropout=config.lstm_r_dropout_rate\n",
    "            ))(conv_dense)\n",
    "            lstm_blocks.append(lstm)\n",
    "\n",
    "        combined = Concatenate()(conv_blocks + lstm_blocks)\n",
    "        attention_layer = AdditiveAttention(use_scale=True)\n",
    "        attention_output = attention_layer([combined, combined], return_attention_scores=False)\n",
    "        context_vector = GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "        dense = Dense(units=config.dense_units, activation='relu')(context_vector)\n",
    "        dropout = Dropout(config.dropout_rate)(dense)\n",
    "        outdata = Dense(self.num_classes, activation='sigmoid')(dropout)\n",
    "        model = Model(inputs=sequence_input, outputs=outdata)\n",
    "        optimizer = Adam(learning_rate=config.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"Beast_model\")\n",
    "\n",
    "# Define the sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # or 'grid', 'random'\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'conv_filter_units': {\n",
    "            'values': [30, 31, 32]\n",
    "        },\n",
    "        'dense_lstm_units': {\n",
    "            'distribution': 'int_uniform',  # Specifies that the values should be integers\n",
    "            'min': 114,  # Minimum value\n",
    "            'max': 125   # Maximum value\n",
    "        },\n",
    "        'lstm_units': {\n",
    "            'distribution': 'int_uniform',  # Specifies that the values should be integers\n",
    "            'min': 100,  # Minimum value\n",
    "            'max': 110   # Maximum value\n",
    "        },\n",
    "        'dense_units': {\n",
    "            'distribution': 'int_uniform',  # Specifies that the values should be integers\n",
    "            'min': 142,  # Minimum value\n",
    "            'max': 152   # Maximum value\n",
    "        },\n",
    "        'lstm_dropout_rate': {\n",
    "            'min': 0.0,\n",
    "            'max': 0.04\n",
    "        },\n",
    "        'lstm_r_dropout_rate': {\n",
    "            'min': 0.01,\n",
    "            'max': 0.06\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform',\n",
    "            'min': -9.21,  # log(1e-4)\n",
    "            'max': -4.61   # log(1e-2)\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'min': 0.0,\n",
    "            'max': 0.1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "input_length = 20\n",
    "num_classes = 1\n",
    "parallel_blocks = 2\n",
    "\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"text_classification\")\n",
    "\n",
    "# Function to train the model\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init(reinit=True)\n",
    "    \n",
    "    hypermodel = TextClassifier_WB(input_length, embeddings_GloVe, num_classes, parallel_blocks)\n",
    "    model = hypermodel.build()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, callbacks=[WandbCallback()])\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 20, 100)              2223500   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 20, 114)              3648      ['conv1d[0][0]']              \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 20, 114)              3648      ['conv1d_1[0][0]']            \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 20, 216)              192672    ['time_distributed[0][0]']    \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 20, 216)              192672    ['time_distributed_1[0][0]']  \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 20, 660)              0         ['time_distributed[0][0]',    \n",
      "                                                                     'time_distributed_1[0][0]',  \n",
      "                                                                     'bidirectional[0][0]',       \n",
      "                                                                     'bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " additive_attention (Additi  (None, 20, 660)              660       ['concatenate[0][0]',         \n",
      " veAttention)                                                        'concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 660)                  0         ['additive_attention[0][0]']  \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 150)                  99150     ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 150)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1)                    151       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2722363 (10.38 MB)\n",
      "Trainable params: 498863 (1.90 MB)\n",
      "Non-trainable params: 2223500 (8.48 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=5\n",
    "\n",
    "KF = KFold(n_splits=n_splits, shuffle=True, random_state=42) # Example: 5-fold cross-validation\n",
    "\n",
    "# Prepare arrays to store results for each fold\n",
    "fold_no = 1\n",
    "loss_per_fold = []\n",
    "acc_per_fold = []\n",
    "\n",
    "for train, test in KF.split(X_train_LSTM, y_train_LSTM):\n",
    "    # Create a fresh model for each fold\n",
    "\n",
    "    # Fit the model\n",
    "    best_model.fit(X_train_LSTM[train], y_train_LSTM[train],\n",
    "                        epochs=n_splits,  # Adjust based on your needs\n",
    "                        batch_size=64,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)],\n",
    "                        validation_data=(X_train_LSTM[test], y_train_LSTM[test]),\n",
    "                        verbose=1)  # You can set verbose to 0 to reduce logs\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = best_model.evaluate(X_train_LSTM[test], y_train_LSTM[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {best_model.metrics_names[0]} of {scores[0]}; {best_model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    loss_per_fold.append(scores[0])\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    fold_no += 1\n",
    "\n",
    "# Print average scores\n",
    "print(f'Average scores for all folds:\\n> Loss: {np.mean(loss_per_fold)}; Accuracy: {np.mean(acc_per_fold)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
