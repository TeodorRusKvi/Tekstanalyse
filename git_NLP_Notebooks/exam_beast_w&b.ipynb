{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.15.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, TimeDistributed, Bidirectional, Concatenate, GlobalAveragePooling1D, AdditiveAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'X_tensorflow.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "X = X_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y = y_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_Notebooks\\wandb\\run-20240418_152212-rx6cfrzt\\files\\model-best\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_Notebooks\\wandb\\run-20240418_152212-rx6cfrzt\\files\\model-best\\assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (c:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_Notebooks\\wandb\\run-20240418_152212-rx6cfrzt\\files\\model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 77s 273ms/step - loss: 0.4514 - accuracy: 0.7892 - val_loss: 0.5206 - val_accuracy: 0.7510\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33424138e69460abbec2cbd9699e21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='61.214 MB of 61.214 MB uploaded (0.135 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▆▇█</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▅▃▂▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆██</td></tr><tr><td>val_loss</td><td>█▄▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.78915</td></tr><tr><td>best_epoch</td><td>4</td></tr><tr><td>best_val_loss</td><td>0.52065</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.45145</td></tr><tr><td>val_accuracy</td><td>0.75104</td></tr><tr><td>val_loss</td><td>0.52065</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cerulean-sweep-3</strong> at: <a href='https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention/runs/rx6cfrzt' target=\"_blank\">https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention/runs/rx6cfrzt</a><br/> View project at: <a href='https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention' target=\"_blank\">https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention</a><br/>Synced 6 W&B file(s), 1 media file(s), 20 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240418_152212-rx6cfrzt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xwt0ra3z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_filter_units: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_units: 103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_rate: 0.08109940785150405\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00014656523099950393\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_dropout_rate: 0.03197132465673359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_r_dropout_rate: 0.01708628534588882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlstm_units: 92\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_Notebooks\\wandb\\run-20240418_152924-xwt0ra3z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention/runs/xwt0ra3z' target=\"_blank\">effortless-sweep-4</a></strong> to <a href='https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention/sweeps/g0bfvfbr' target=\"_blank\">https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention/sweeps/g0bfvfbr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention' target=\"_blank\">https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention/sweeps/g0bfvfbr' target=\"_blank\">https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention/sweeps/g0bfvfbr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention/runs/xwt0ra3z' target=\"_blank\">https://wandb.ai/teodor-ruskvi/LSTM_CNN_Attention/runs/xwt0ra3z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.6412 - accuracy: 0.6474"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import tensorflow as tf\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, LSTM, Dropout, TimeDistributed, Bidirectional, Concatenate, GlobalAveragePooling1D, AdditiveAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "class TextClassifierHyperModel:\n",
    "    def __init__(self, input_shape, embeddings_GloVe, num_classes, parallel_blocks):\n",
    "        self.input_shape = input_shape\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "        self.num_classes = num_classes\n",
    "        self.parallel_blocks = parallel_blocks\n",
    "\n",
    "    def build(self):\n",
    "        config = wandb.config\n",
    "        sequence_input = Input(shape=(self.input_shape,), dtype='int32')\n",
    "        embedded_sequences = Embedding(input_dim=self.embeddings_GloVe.shape[0],\n",
    "                                       output_dim=self.embeddings_GloVe.shape[1],\n",
    "                                       weights=[self.embeddings_GloVe],\n",
    "                                       trainable=False)(sequence_input)\n",
    "\n",
    "        conv_blocks = []\n",
    "        lstm_blocks = []\n",
    "\n",
    "        for _ in range(self.parallel_blocks):\n",
    "            conv = Conv1D(\n",
    "                filters=config.conv_filter_units,\n",
    "                kernel_size=1,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                strides=1)(embedded_sequences)\n",
    "            conv_dense = TimeDistributed(Dense(config.dense_units, activation='relu'))(conv)\n",
    "            conv_blocks.append(conv_dense)\n",
    "\n",
    "            lstm = Bidirectional(LSTM(\n",
    "                units=config.lstm_units,\n",
    "                return_sequences=True,\n",
    "                dropout=config.lstm_dropout_rate,\n",
    "                recurrent_dropout=config.lstm_r_dropout_rate\n",
    "            ))(conv_dense)\n",
    "            lstm_blocks.append(lstm)\n",
    "\n",
    "        combined = Concatenate()(conv_blocks + lstm_blocks)\n",
    "        attention_layer = AdditiveAttention(use_scale=True)\n",
    "        attention_output = attention_layer([combined, combined], return_attention_scores=False)\n",
    "        context_vector = GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "        dense = Dense(units=config.dense_units, activation='relu')(context_vector)\n",
    "        dropout = Dropout(config.dropout_rate)(dense)\n",
    "        outdata = Dense(self.num_classes, activation='sigmoid')(dropout)\n",
    "        model = Model(inputs=sequence_input, outputs=outdata)\n",
    "        optimizer = Adam(learning_rate=config.learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=\"LSTM_CNN_Attention\")\n",
    "\n",
    "# Define the sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'random',  # or 'grid', 'random'\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'conv_filter_units': {\n",
    "            'values': [30, 31, 32]\n",
    "        },\n",
    "        'dense_units': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 60,  # Minimum verdi\n",
    "            'max': 160  # Maksimum verdi\n",
    "        },\n",
    "        'lstm_units': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 60,  # Minimum verdi\n",
    "            'max': 160  # Maksimum verdi\n",
    "        },\n",
    "        'lstm_dropout_rate': {\n",
    "            'min': 0.0,\n",
    "            'max': 0.04\n",
    "        },\n",
    "        'lstm_r_dropout_rate': {\n",
    "            'min': 0.01,\n",
    "            'max': 0.06\n",
    "        },\n",
    "        'dense_units':      {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 60,  # Minimum verdi\n",
    "            'max': 160  # Maksimum verdi\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform',\n",
    "            'min': -9.21,  # log(1e-4)\n",
    "            'max': -4.61   # log(1e-2)\n",
    "        },\n",
    "        'dropout_rate': {\n",
    "            'min': 0.0,\n",
    "            'max': 0.1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "input_length = 20\n",
    "num_classes = 1\n",
    "parallel_blocks = 2\n",
    "\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"LSTM_CNN_Attention\")\n",
    "\n",
    "# Function to train the model\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init(reinit=True)\n",
    "    \n",
    "    hypermodel = TextClassifierHyperModel(input_length, embeddings_GloVe, num_classes, parallel_blocks)\n",
    "    model = hypermodel.build()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks=[WandbCallback()])\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 20, 100)              2223500   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 20, 114)              3648      ['conv1d[0][0]']              \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 20, 114)              3648      ['conv1d_1[0][0]']            \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 20, 216)              192672    ['time_distributed[0][0]']    \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 20, 216)              192672    ['time_distributed_1[0][0]']  \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 20, 660)              0         ['time_distributed[0][0]',    \n",
      "                                                                     'time_distributed_1[0][0]',  \n",
      "                                                                     'bidirectional[0][0]',       \n",
      "                                                                     'bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " additive_attention (Additi  (None, 20, 660)              660       ['concatenate[0][0]',         \n",
      " veAttention)                                                        'concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 660)                  0         ['additive_attention[0][0]']  \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 150)                  99150     ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 150)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1)                    151       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2722363 (10.38 MB)\n",
      "Trainable params: 498863 (1.90 MB)\n",
      "Non-trainable params: 2223500 (8.48 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
