{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Last inn 'X_train_LSTM' fra en CSV-fil\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url_data\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_tensorflow.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Konverter hele DataFrame til et NumPy array\u001b[39;00m\n\u001b[0;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m X_df\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_df = pd.read_csv(url_data+'X_tensorflow.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "X = X_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y = y_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "print('Shape of label tensor:', y.shape)\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of embeddings_GloVe:', embeddings_GloVe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "#batch_size = None\n",
    "#batch_size=64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(20,)))\n",
    "# Adding the Embedding layer with pre-trained weights and specifying input_length\n",
    "model.add(Embedding(input_dim=embeddings_GloVe.shape[0],\n",
    "                    output_dim=embeddings_GloVe.shape[1],\n",
    "                    weights=[embeddings_GloVe],\n",
    "                    trainable=False))\n",
    "\n",
    "model.add(Bidirectional(LSTM(64, # Number of hidden states, number of reccurent units for each vector\n",
    "    activation='tanh',\n",
    "    recurrent_activation='sigmoid',\n",
    "    recurrent_dropout=0.2,\n",
    "    return_sequences=False)))\n",
    "\n",
    "model.add(Dense(77, activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)\n",
    "\n",
    "\n",
    "# history = model.fit(X_train_LSTM, y_train_LSTM,\n",
    "#                     epochs=20, # Adjust based on your needs\n",
    "#                     validation_split=0.2,\n",
    "#                     batch_size=64,\n",
    "#                     callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"text_classification\")\n",
    "\n",
    "# Function to train the model\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init(reinit=True)\n",
    "    \n",
    "    hypermodel = TextClassifier_WB(input_length, embeddings_GloVe, num_classes, parallel_blocks)\n",
    "    model = hypermodel.build()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=5, callbacks=[WandbCallback()])\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "with plt.xkcd():\n",
    "    plt.plot(model_1.history['accuracy'], label='train_acc')\n",
    "    plt.plot(model_1.history['val_accuracy'], label='val_acc')\n",
    "    plt.plot(model_1.history['loss'], label='train_loss')\n",
    "    plt.plot(model_1.history['val_loss'], label='val_loss')\n",
    "\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Training results on initial model')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With POS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, SpatialDropout1D, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout, concatenate\n",
    "\n",
    "# Text input pathway\n",
    "text_input = Input(shape=(max_len,), dtype='int32', name='text_input')  # Ensure the shape parameter matches your input length\n",
    "text_embedded = Embedding(input_dim=vocab_size+1,\n",
    "                          output_dim=embedding_dim,\n",
    "                          input_length=max_len,\n",
    "                          weights=[embeddings_GloVe],\n",
    "                          trainable=False)(text_input)\n",
    "text_dropout = SpatialDropout1D(0.2)(text_embedded)\n",
    "text_conv = Conv1D(filters=32, kernel_size=5, activation='relu')(text_dropout)\n",
    "text_pool = MaxPooling1D(pool_size=4)(text_conv)\n",
    "text_lstm = Bidirectional(LSTM(64))(text_pool)\n",
    "\n",
    "# POS tags input pathway\n",
    "pos_input = Input(shape=(max_len,), dtype='int32', name='pos_input')\n",
    "pos_embedded = Embedding(input_dim=POS_size+1, output_dim=100, input_length=max_len)(pos_input)\n",
    "pos_dropout = SpatialDropout1D(0.2)(pos_embedded)\n",
    "pos_conv = Conv1D(filters=32, kernel_size=5, activation='relu')(pos_dropout)\n",
    "pos_pool = MaxPooling1D(pool_size=4)(pos_conv)\n",
    "pos_lstm = Bidirectional(LSTM(32))(pos_pool)\n",
    "\n",
    "# Combine the pathways\n",
    "combined = concatenate([text_lstm, pos_lstm])\n",
    "\n",
    "# Classification layers\n",
    "layer = Dense(128, activation='relu')(combined)\n",
    "layer = Dropout(0.5)(layer)\n",
    "output = Dense(2, activation='softmax')(layer)\n",
    "\n",
    "# Build and compile the model\n",
    "POS_text_model = Model(inputs=[text_input, pos_input], outputs=output)\n",
    "POS_text_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], run_eagerly=False)\n",
    "\n",
    "# Model summary\n",
    "POS_text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.xkcd():\n",
    "    plt.plot(POS_text_model.history['accuracy'], label='train_acc')\n",
    "    plt.plot(POS_text_model.history['val_accuracy'], label='val_acc')\n",
    "    plt.plot(POS_text_model.history['loss'], label='train_loss')\n",
    "    plt.plot(POS_text_model.history['val_loss'], label='val_loss')\n",
    "\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Training results on initial model')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
