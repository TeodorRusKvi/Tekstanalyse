{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.15.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Concatenate, Dropout, Dense, Layer, Multiply, TimeDistributed, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras_tuner import HyperModel, RandomSearch, BayesianOptimization, Hyperband\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22234 unique tokens.\n",
      "\n",
      "First 10 is listen below:\n",
      "{'<OOV>': 1, 'people': 2, 'like': 3, 'work': 4, 'right': 5, 'trump': 6, 'think': 7, 'state': 8, 'government': 9, 'party': 10}\n"
     ]
    }
   ],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "df = pd.read_csv(url_data + 'new_df.csv')\n",
    "\n",
    "df['All_text'] = df['All_text'].replace(['U.S.', 'U.S.A.'], ['US', 'USA'], regex=True)\n",
    "df['Processed'] = df['Processed'].fillna(0)\n",
    "df['Processed'] = df['Processed'].astype(str)\n",
    "df['All_text'] = df['All_text'].fillna(0)\n",
    "df['All_text'] = df['All_text'].astype(str)\n",
    "\n",
    "# df.to_csv('new_df.csv', index=False)\n",
    "\n",
    "# Making the relevant columns to lists\n",
    "all_texts = (df['All_text'].to_list())\n",
    "texts = df['Processed'].to_list()\n",
    "\n",
    "# Setting the wanted text for further modelling\n",
    "corpus = texts\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>') # Hyperparameters = num_words=vocab_size, oov_token=oov_tok\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.\\n\\nFirst 10 is listen below:')\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All_text</th>\n",
       "      <th>Political_Lean</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>subreddit_encoded</th>\n",
       "      <th>Processed</th>\n",
       "      <th>Dependency_Tags</th>\n",
       "      <th>POS_Tags</th>\n",
       "      <th>Named_Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No matter who someone is, how they look like, ...</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>socialism</td>\n",
       "      <td>14</td>\n",
       "      <td>matter look like language speak wear remember ...</td>\n",
       "      <td>['advmod', 'ROOT', 'prep', 'compound', 'compou...</td>\n",
       "      <td>['ADV', 'VERB', 'ADP', 'NOUN', 'NOUN', 'NOUN',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biden speech draws 38.2 million US TV viewers 0</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>democrats</td>\n",
       "      <td>10</td>\n",
       "      <td>biden speech draw million U.S. tv viewer</td>\n",
       "      <td>['compound', 'nsubj', 'ROOT', 'nummod', 'compo...</td>\n",
       "      <td>['PROPN', 'NOUN', 'VERB', 'NUM', 'PROPN', 'NOU...</td>\n",
       "      <td>['biden speech draw million (ORG)', 'U.S. (GPE)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>State of the union Who watched the state of th...</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>DemocraticSocialism</td>\n",
       "      <td>2</td>\n",
       "      <td>state union watch state union night opinion</td>\n",
       "      <td>['compound', 'nsubj', 'ROOT', 'compound', 'com...</td>\n",
       "      <td>['NOUN', 'PROPN', 'VERB', 'NOUN', 'PROPN', 'NO...</td>\n",
       "      <td>['state union watch state union (ORG)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We Should Just Give Poor People Money 0</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>SocialDemocracy</td>\n",
       "      <td>6</td>\n",
       "      <td>poor people money</td>\n",
       "      <td>['amod', 'compound', 'ROOT']</td>\n",
       "      <td>['ADJ', 'NOUN', 'NOUN']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do it for the Dew 0</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>democrats</td>\n",
       "      <td>10</td>\n",
       "      <td>dew</td>\n",
       "      <td>['ROOT']</td>\n",
       "      <td>['NOUN']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12849</th>\n",
       "      <td>Ron Paul’s Spirited Defense of WikiLeaks &amp; Fre...</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>anarchocapitalism</td>\n",
       "      <td>8</td>\n",
       "      <td>ron paul spirited defense wikileaks free infor...</td>\n",
       "      <td>['compound', 'nsubj', 'amod', 'compound', 'nsu...</td>\n",
       "      <td>['PROPN', 'PROPN', 'VERB', 'NOUN', 'NOUN', 'AD...</td>\n",
       "      <td>['ron paul (PERSON)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12850</th>\n",
       "      <td>“Anarcho-capitalism, in my opinion, is a doctr...</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>anarchocapitalism</td>\n",
       "      <td>8</td>\n",
       "      <td>anarcho capitalism opinion doctrinal system im...</td>\n",
       "      <td>['nmod', 'compound', 'compound', 'amod', 'nsub...</td>\n",
       "      <td>['PROPN', 'PROPN', 'NOUN', 'ADJ', 'NOUN', 'VER...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12851</th>\n",
       "      <td>Mises Wiki is a wiki project dedicated to the ...</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>anarchocapitalism</td>\n",
       "      <td>8</td>\n",
       "      <td>mises wiki wiki project dedicate advancement a...</td>\n",
       "      <td>['ROOT', 'compound', 'compound', 'nmod', 'amod...</td>\n",
       "      <td>['VERB', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'NOUN'...</td>\n",
       "      <td>['mises wiki wiki (PERSON)', 'austrian (NORP)'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12852</th>\n",
       "      <td>Fireman Protection Monopoly - Is This Failed C...</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>anarchocapitalism</td>\n",
       "      <td>8</td>\n",
       "      <td>fireman protection monopoly failed capitalism</td>\n",
       "      <td>['compound', 'compound', 'nsubj', 'ROOT', 'dobj']</td>\n",
       "      <td>['PROPN', 'PROPN', 'NOUN', 'VERB', 'NOUN']</td>\n",
       "      <td>['fireman protection monopoly (ORG)']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12853</th>\n",
       "      <td>Can this Wikipedia Article be Better Written? ...</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>anarchocapitalism</td>\n",
       "      <td>8</td>\n",
       "      <td>wikipedia article better write listen writing ...</td>\n",
       "      <td>['compound', 'nsubj', 'advmod', 'ROOT', 'dobj'...</td>\n",
       "      <td>['NOUN', 'NOUN', 'ADV', 'VERB', 'NOUN', 'VERB'...</td>\n",
       "      <td>['wikipedia article (ORG)']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12854 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                All_text Political_Lean  \\\n",
       "0      No matter who someone is, how they look like, ...        Liberal   \n",
       "1        Biden speech draws 38.2 million US TV viewers 0        Liberal   \n",
       "2      State of the union Who watched the state of th...        Liberal   \n",
       "3                We Should Just Give Poor People Money 0        Liberal   \n",
       "4                                    Do it for the Dew 0        Liberal   \n",
       "...                                                  ...            ...   \n",
       "12849  Ron Paul’s Spirited Defense of WikiLeaks & Fre...   Conservative   \n",
       "12850  “Anarcho-capitalism, in my opinion, is a doctr...   Conservative   \n",
       "12851  Mises Wiki is a wiki project dedicated to the ...   Conservative   \n",
       "12852  Fireman Protection Monopoly - Is This Failed C...   Conservative   \n",
       "12853  Can this Wikipedia Article be Better Written? ...   Conservative   \n",
       "\n",
       "                 Subreddit  subreddit_encoded  \\\n",
       "0                socialism                 14   \n",
       "1                democrats                 10   \n",
       "2      DemocraticSocialism                  2   \n",
       "3          SocialDemocracy                  6   \n",
       "4                democrats                 10   \n",
       "...                    ...                ...   \n",
       "12849    anarchocapitalism                  8   \n",
       "12850    anarchocapitalism                  8   \n",
       "12851    anarchocapitalism                  8   \n",
       "12852    anarchocapitalism                  8   \n",
       "12853    anarchocapitalism                  8   \n",
       "\n",
       "                                               Processed  \\\n",
       "0      matter look like language speak wear remember ...   \n",
       "1               biden speech draw million U.S. tv viewer   \n",
       "2            state union watch state union night opinion   \n",
       "3                                      poor people money   \n",
       "4                                                    dew   \n",
       "...                                                  ...   \n",
       "12849  ron paul spirited defense wikileaks free infor...   \n",
       "12850  anarcho capitalism opinion doctrinal system im...   \n",
       "12851  mises wiki wiki project dedicate advancement a...   \n",
       "12852      fireman protection monopoly failed capitalism   \n",
       "12853  wikipedia article better write listen writing ...   \n",
       "\n",
       "                                         Dependency_Tags  \\\n",
       "0      ['advmod', 'ROOT', 'prep', 'compound', 'compou...   \n",
       "1      ['compound', 'nsubj', 'ROOT', 'nummod', 'compo...   \n",
       "2      ['compound', 'nsubj', 'ROOT', 'compound', 'com...   \n",
       "3                           ['amod', 'compound', 'ROOT']   \n",
       "4                                               ['ROOT']   \n",
       "...                                                  ...   \n",
       "12849  ['compound', 'nsubj', 'amod', 'compound', 'nsu...   \n",
       "12850  ['nmod', 'compound', 'compound', 'amod', 'nsub...   \n",
       "12851  ['ROOT', 'compound', 'compound', 'nmod', 'amod...   \n",
       "12852  ['compound', 'compound', 'nsubj', 'ROOT', 'dobj']   \n",
       "12853  ['compound', 'nsubj', 'advmod', 'ROOT', 'dobj'...   \n",
       "\n",
       "                                                POS_Tags  \\\n",
       "0      ['ADV', 'VERB', 'ADP', 'NOUN', 'NOUN', 'NOUN',...   \n",
       "1      ['PROPN', 'NOUN', 'VERB', 'NUM', 'PROPN', 'NOU...   \n",
       "2      ['NOUN', 'PROPN', 'VERB', 'NOUN', 'PROPN', 'NO...   \n",
       "3                                ['ADJ', 'NOUN', 'NOUN']   \n",
       "4                                               ['NOUN']   \n",
       "...                                                  ...   \n",
       "12849  ['PROPN', 'PROPN', 'VERB', 'NOUN', 'NOUN', 'AD...   \n",
       "12850  ['PROPN', 'PROPN', 'NOUN', 'ADJ', 'NOUN', 'VER...   \n",
       "12851  ['VERB', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'NOUN'...   \n",
       "12852         ['PROPN', 'PROPN', 'NOUN', 'VERB', 'NOUN']   \n",
       "12853  ['NOUN', 'NOUN', 'ADV', 'VERB', 'NOUN', 'VERB'...   \n",
       "\n",
       "                                          Named_Entities  \n",
       "0                                                     []  \n",
       "1      ['biden speech draw million (ORG)', 'U.S. (GPE)']  \n",
       "2                ['state union watch state union (ORG)']  \n",
       "3                                                     []  \n",
       "4                                                     []  \n",
       "...                                                  ...  \n",
       "12849                              ['ron paul (PERSON)']  \n",
       "12850                                                 []  \n",
       "12851  ['mises wiki wiki (PERSON)', 'austrian (NORP)'...  \n",
       "12852              ['fireman protection monopoly (ORG)']  \n",
       "12853                        ['wikipedia article (ORG)']  \n",
       "\n",
       "[12854 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_train_LSTM = pd.read_csv(url_data+'X_tensorflow.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "X_train_LSTM = X_train_LSTM.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_train_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y_train_LSTM = y_train_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_LSTM, y_train_LSTM, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6008: logdir logs/fit (started 5 days, 14:01:39 ago; pid 1956)\n",
      "  - port 6006: logdir logs/hparam_tuning (started 9 days, 14:48:52 ago; pid 23884)\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters with TensorBoard HParams API\n",
    "HP_FILTERS = hp.HParam('filters', hp.Discrete([32, 35]))\n",
    "HP_NUM_UNITS2 = hp.HParam('num_units2', hp.Discrete([64, 128, 152]))\n",
    "HP_DROPOUT2 = hp.HParam('dropout2', hp.RealInterval(0.2, 0.6))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.001, 0.01))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 1956), started 5 days, 14:01:39 ago. (Use '!kill 1956' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c2e751437214ad82\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c2e751437214ad82\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.layers import AdditiveAttention, Attention\n",
    "\n",
    "\n",
    "class Attention(Layer): #BahdanauAttention\n",
    "    def __init__(self,**kwargs):\n",
    "        super(Attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(Attention,self).get_config()\n",
    "\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_len, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_len, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Softmax, Dense\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # Divide channels for query and key\n",
    "        self.query = Dense(channels // 8, use_bias=False, kernel_initializer='he_normal')\n",
    "        self.key = Dense(channels // 8, use_bias=False, kernel_initializer='he_normal')\n",
    "        self.value = Dense(channels, use_bias=False, kernel_initializer='he_normal')\n",
    "        \n",
    "        # Initialize gamma as a trainable parameter\n",
    "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer=Constant(0.0), trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Shape of x is (batch, width, channels)\n",
    "        # Learn query, key, and value vectors\n",
    "        f = self.query(x)  # Shape (batch, width, channels/8)\n",
    "        g = self.key(x)    # Shape (batch, width, channels/8)\n",
    "        h = self.value(x)  # Shape (batch, width, channels)\n",
    "        \n",
    "        # Transpose and multiply to get the attention scores\n",
    "        s = tf.matmul(f, g, transpose_b=True)  # Shape (batch, width, width)\n",
    "        beta = Softmax(axis=-1)(s)  # Softmax over last dimension to get attention weights\n",
    "        \n",
    "        # Apply attention weights to value vector\n",
    "        o = tf.matmul(beta, h)  # Shape (batch, width, channels)\n",
    "        # Apply gamma and add input (residual connection)\n",
    "        return self.gamma * o + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "31                |31                |conv_filter_units\n",
      "115               |115               |lstm_dense_units\n",
      "101               |101               |lstm_units\n",
      "0.02              |0.02              |lstm_dropout_rate\n",
      "0.01              |0.01              |lstm_r_dropout_rate\n",
      "144               |144               |dense_units\n",
      "0.0083579         |0.0083579         |learning_rate\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TextClassifierHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, embeddings_GloVe, num_classes, parallel_blocks):#, include_attention_weights=False):\n",
    "        self.input_shape = input_shape\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "        self.num_classes = num_classes\n",
    "        self.parallel_blocks = parallel_blocks\n",
    "        #self.include_attention_weights = include_attention_weights\n",
    "\n",
    "\n",
    "    def build(self, hp):\n",
    "        sequence_input = Input(shape=(self.input_shape,), dtype='int32')\n",
    "        embedded_sequences = Embedding(input_dim=self.embeddings_GloVe.shape[0],\n",
    "                                       output_dim=self.embeddings_GloVe.shape[1],\n",
    "                                       weights=[self.embeddings_GloVe],\n",
    "                                       trainable=False)(sequence_input)\n",
    "                                    #    embeddings_regularizer=keras.regularizers.l2(hp.Float('L2_rate', \n",
    "                                    #                              min_value=0.0, \n",
    "                                    #                              max_value=0.2, \n",
    "                                    #                             step=0.3))\n",
    "                                       \n",
    "            \n",
    "\n",
    "        conv_blocks = []\n",
    "        lstm_blocks = []\n",
    "\n",
    "        for _ in range (self.parallel_blocks): #(hp.Int('blocks', 1, self.parallel_blocks)):\n",
    "            conv = Conv1D(\n",
    "                filters=hp.Int('conv_filter_units', min_value=30, max_value=32, step=1),\n",
    "                kernel_size= 1, #hp.Int('conv_kernel_size', min_value=1, max_value=3, step=1),\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                strides=1)(embedded_sequences)\n",
    "                \n",
    "                                                                                            \n",
    "            # Apply a TimeDistributed Dense layer to each timestep of the convolutional block's output\n",
    "            conv_dense = TimeDistributed(Dense(hp.Int('lstm_dense_units', min_value=114, max_value=125, step=1), activation='relu'))(conv)\n",
    "            conv_blocks.append(conv_dense)\n",
    "\n",
    "            lstm = Bidirectional(LSTM(\n",
    "                units=hp.Int('lstm_units', min_value=100, max_value=110, step=1),\n",
    "                return_sequences=True,\n",
    "                dropout=hp.Float('lstm_dropout_rate', min_value=0.0, max_value=0.04, step=0.01),\n",
    "                recurrent_dropout=hp.Float('lstm_r_dropout_rate', min_value=0.01, max_value=0.06, step=0.01)\n",
    "            ))(conv_dense)  # Pass the output of the Dense layer into LSTM\n",
    "            lstm_blocks.append(lstm)\n",
    "\n",
    "        combined = Concatenate()(conv_blocks + lstm_blocks)\n",
    "        # Apply the custom attention mechanism\n",
    "        attention_layer = AdditiveAttention(use_scale=True)\n",
    "        attention_output = attention_layer([combined, combined], return_attention_scores=False)\n",
    "        context_vector = GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "        dense = Dense(units=hp.Int('dense_units', min_value=142, max_value=152, step=1), activation='relu')(context_vector)\n",
    "        dropout = Dropout(0.1)(dense) #Dropout(hp.Float('dropout_rate', min_value=0.0, max_value=0.2, step=0.1))\n",
    "        outdata = Dense(self.num_classes, activation='sigmoid')(dropout)\n",
    "        output = (outdata)\n",
    "        model = Model(inputs=sequence_input, outputs=output)\n",
    "        \n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# Note: When actually using this model for training in tuner, make sure to remove attention_weights from outputs.\n",
    "# Hyperparameters and settings\n",
    "input_length = 20\n",
    "num_classes = 1\n",
    "parallel_blocks = 2\n",
    "log_dir = 'logs/fit/' + datetime.now().strftime(\"%d-%m-%Y %H-%M-%S\")\n",
    "\n",
    "# Use this for training (without attention weights in outputs)\n",
    "hypermodel_for_training = TextClassifierHyperModel(input_length, embeddings_GloVe, num_classes, parallel_blocks)#, include_attention_weights=False)\n",
    "\n",
    "\n",
    "# # Use this for visualization (with attention weights in outputs)\n",
    "# hypermodel_for_visualization = TextClassifierHyperModel(input_length, embeddings_GloVe, num_classes, parallel_blocks, include_attention_weights=True)\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    hypermodel_for_training,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Set the maximum number of trials (model configurations to test)\n",
    "    executions_per_trial=1,  # Number of models that should be built and fit for each trial\n",
    "    directory=log_dir,\n",
    "    project_name='TextClassification'\n",
    ")\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0,  # No histogram computation, set to 1 or higher to compute histograms every '1' epoch or specified frequency\n",
    "    update_freq='epoch'  # Log metrics and histograms every epoch (default), not every batch\n",
    ")\n",
    "\n",
    "# Setup the checkpoint location\n",
    "checkpoint_filepath = '/kaggle/working/best_model.h5'\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "model_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "\n",
    "# Assume X_train, y_train, X_val, y_val are defined\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=5, \n",
    "             validation_data=(X_test, y_test),\n",
    "             callbacks=[tensorboard_callback, model_callback])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 20, 100)              2223500   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 20, 114)              3648      ['conv1d[0][0]']              \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 20, 114)              3648      ['conv1d_1[0][0]']            \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 20, 216)              192672    ['time_distributed[0][0]']    \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 20, 216)              192672    ['time_distributed_1[0][0]']  \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 20, 660)              0         ['time_distributed[0][0]',    \n",
      "                                                                     'time_distributed_1[0][0]',  \n",
      "                                                                     'bidirectional[0][0]',       \n",
      "                                                                     'bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " additive_attention (Additi  (None, 20, 660)              660       ['concatenate[0][0]',         \n",
      " veAttention)                                                        'concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 660)                  0         ['additive_attention[0][0]']  \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 150)                  99150     ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 150)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1)                    151       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2722363 (10.38 MB)\n",
      "Trainable params: 498863 (1.90 MB)\n",
      "Non-trainable params: 2223500 (8.48 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=5\n",
    "\n",
    "KF = KFold(n_splits=n_splits, shuffle=True, random_state=42) # Example: 5-fold cross-validation\n",
    "\n",
    "# Prepare arrays to store results for each fold\n",
    "fold_no = 1\n",
    "loss_per_fold = []\n",
    "acc_per_fold = []\n",
    "\n",
    "for train, test in KF.split(X_train_LSTM, y_train_LSTM):\n",
    "    # Create a fresh model for each fold\n",
    "\n",
    "    # Fit the model\n",
    "    best_model.fit(X_train_LSTM[train], y_train_LSTM[train],\n",
    "                        epochs=n_splits,  # Adjust based on your needs\n",
    "                        batch_size=64,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)],\n",
    "                        validation_data=(X_train_LSTM[test], y_train_LSTM[test]),\n",
    "                        verbose=1)  # You can set verbose to 0 to reduce logs\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = best_model.evaluate(X_train_LSTM[test], y_train_LSTM[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {best_model.metrics_names[0]} of {scores[0]}; {best_model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    loss_per_fold.append(scores[0])\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    fold_no += 1\n",
    "\n",
    "# Print average scores\n",
    "print(f'Average scores for all folds:\\n> Loss: {np.mean(loss_per_fold)}; Accuracy: {np.mean(acc_per_fold)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
