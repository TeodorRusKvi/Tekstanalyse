{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow==2.15.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Concatenate, Dropout, Dense, Layer, Multiply, TimeDistributed, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras_tuner import HyperModel, RandomSearch, BayesianOptimization, Hyperband\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22234 unique tokens.\n",
      "\n",
      "First 10 is listen below:\n",
      "{'<OOV>': 1, 'people': 2, 'like': 3, 'work': 4, 'right': 5, 'trump': 6, 'think': 7, 'state': 8, 'government': 9, 'party': 10}\n"
     ]
    }
   ],
   "source": [
    "url_data = 'https://raw.githubusercontent.com/TeodorRusKvi/Tekstanalyse/main/git_NLP_data/'\n",
    "\n",
    "df = pd.read_csv(url_data + 'new_df.csv')\n",
    "\n",
    "df['All_text'] = df['All_text'].replace(['U.S.', 'U.S.A.'], ['US', 'USA'], regex=True)\n",
    "df['Processed'] = df['Processed'].fillna(0)\n",
    "df['Processed'] = df['Processed'].astype(str)\n",
    "df['All_text'] = df['All_text'].fillna(0)\n",
    "df['All_text'] = df['All_text'].astype(str)\n",
    "\n",
    "# df.to_csv('new_df.csv', index=False)\n",
    "\n",
    "# Making the relevant columns to lists\n",
    "all_texts = (df['All_text'].to_list())\n",
    "texts = df['Processed'].to_list()\n",
    "\n",
    "# Setting the wanted text for further modelling\n",
    "corpus = texts\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>') # Hyperparameters = num_words=vocab_size, oov_token=oov_tok\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.\\n\\nFirst 10 is listen below:')\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\git_NLP_data\\file_name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Political Lean\n",
       "Liberal         8319\n",
       "Conservative    4535\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "political_lean_counts = df['Political Lean'].value_counts()\n",
    "political_lean_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last inn 'X_train_LSTM' fra en CSV-fil\n",
    "X_train_LSTM = pd.read_csv(url_data+'X_tensorflow.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "X_train_LSTM = X_train_LSTM.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "y_train_df = pd.read_csv(url_data+'y_liberal.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "y_train_LSTM = y_train_df.to_numpy()\n",
    "\n",
    "# Last inn 'y_train_LSTM' fra en CSV-fil\n",
    "embeddings_GloVe = pd.read_csv(url_data+'embeddings_glove.csv')\n",
    "# Konverter hele DataFrame til et NumPy array\n",
    "embeddings_GloVe = embeddings_GloVe.to_numpy()\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_LSTM, y_train_LSTM, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known TensorBoard instances:\n",
      "  - port 6008: logdir logs/fit (started 4 days, 2:01:37 ago; pid 1956)\n",
      "  - port 6006: logdir logs/hparam_tuning (started 8 days, 2:48:50 ago; pid 23884)\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters with TensorBoard HParams API\n",
    "HP_FILTERS = hp.HParam('filters', hp.Discrete([32, 35]))\n",
    "HP_NUM_UNITS2 = hp.HParam('num_units2', hp.Discrete([64, 128, 152]))\n",
    "HP_DROPOUT2 = hp.HParam('dropout2', hp.RealInterval(0.2, 0.6))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.001, 0.01))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', 'rmsprop']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 1956), started 4 days, 2:01:37 ago. (Use '!kill 1956' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f27798eba2af51a1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f27798eba2af51a1\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.layers import AdditiveAttention, Attention\n",
    "\n",
    "\n",
    "class Attention(Layer): #BahdanauAttention\n",
    "    def __init__(self,**kwargs):\n",
    "        super(Attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(Attention,self).get_config()\n",
    "\n",
    "\n",
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_len, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_len, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_len, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Softmax, Dense\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, channels):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # Divide channels for query and key\n",
    "        self.query = Dense(channels // 8, use_bias=False, kernel_initializer='he_normal')\n",
    "        self.key = Dense(channels // 8, use_bias=False, kernel_initializer='he_normal')\n",
    "        self.value = Dense(channels, use_bias=False, kernel_initializer='he_normal')\n",
    "        \n",
    "        # Initialize gamma as a trainable parameter\n",
    "        self.gamma = self.add_weight(name='gamma', shape=[1], initializer=Constant(0.0), trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Shape of x is (batch, width, channels)\n",
    "        # Learn query, key, and value vectors\n",
    "        f = self.query(x)  # Shape (batch, width, channels/8)\n",
    "        g = self.key(x)    # Shape (batch, width, channels/8)\n",
    "        h = self.value(x)  # Shape (batch, width, channels)\n",
    "        \n",
    "        # Transpose and multiply to get the attention scores\n",
    "        s = tf.matmul(f, g, transpose_b=True)  # Shape (batch, width, width)\n",
    "        beta = Softmax(axis=-1)(s)  # Softmax over last dimension to get attention weights\n",
    "        \n",
    "        # Apply attention weights to value vector\n",
    "        o = tf.matmul(beta, h)  # Shape (batch, width, channels)\n",
    "        # Apply gamma and add input (residual connection)\n",
    "        return self.gamma * o + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 03m 41s]\n",
      "val_accuracy: 0.7432905435562134\n",
      "\n",
      "Best val_accuracy So Far: 0.7670167088508606\n",
      "Total elapsed time: 00h 35m 29s\n",
      "WARNING:tensorflow:From c:\\Users\\bugat\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\legacy\\save.py:538: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TextClassifierHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, embeddings_GloVe, num_classes, parallel_blocks):#, include_attention_weights=False):\n",
    "        self.input_shape = input_shape\n",
    "        self.embeddings_GloVe = embeddings_GloVe\n",
    "        self.num_classes = num_classes\n",
    "        self.parallel_blocks = parallel_blocks\n",
    "        #self.include_attention_weights = include_attention_weights\n",
    "\n",
    "\n",
    "    def build(self, hp):\n",
    "        sequence_input = Input(shape=(self.input_shape,), dtype='int32')\n",
    "        embedded_sequences = Embedding(input_dim=self.embeddings_GloVe.shape[0],\n",
    "                                       output_dim=self.embeddings_GloVe.shape[1],\n",
    "                                       weights=[self.embeddings_GloVe],\n",
    "                                       trainable=False)(sequence_input)\n",
    "                                    #    embeddings_regularizer=keras.regularizers.l2(hp.Float('L2_rate', \n",
    "                                    #                              min_value=0.0, \n",
    "                                    #                              max_value=0.2, \n",
    "                                    #                             step=0.3))\n",
    "                                       \n",
    "            \n",
    "\n",
    "        conv_blocks = []\n",
    "        lstm_blocks = []\n",
    "\n",
    "        for _ in range (self.parallel_blocks): #(hp.Int('blocks', 1, self.parallel_blocks)):\n",
    "            conv = Conv1D(\n",
    "                filters=hp.Int('conv_filter_units', min_value=30, max_value=32, step=1),\n",
    "                kernel_size= 1, #hp.Int('conv_kernel_size', min_value=1, max_value=3, step=1),\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                strides=1)(embedded_sequences)\n",
    "                \n",
    "                                                                                            \n",
    "            # Apply a TimeDistributed Dense layer to each timestep of the convolutional block's output\n",
    "            conv_dense = TimeDistributed(Dense(hp.Int('lstm_dense_units', min_value=114, max_value=125, step=1), activation='relu'))(conv)\n",
    "            conv_blocks.append(conv_dense)\n",
    "\n",
    "            lstm = Bidirectional(LSTM(\n",
    "                units=hp.Int('lstm_units', min_value=100, max_value=110, step=1),\n",
    "                return_sequences=True,\n",
    "                dropout=hp.Float('lstm_dropout_rate', min_value=0.0, max_value=0.04, step=0.01),\n",
    "                recurrent_dropout=hp.Float('lstm_r_dropout_rate', min_value=0.01, max_value=0.06, step=0.01)\n",
    "            ))(conv_dense)  # Pass the output of the Dense layer into LSTM\n",
    "            lstm_blocks.append(lstm)\n",
    "\n",
    "        combined = Concatenate()(conv_blocks + lstm_blocks)\n",
    "        # Apply the custom attention mechanism\n",
    "        attention_layer = AdditiveAttention(use_scale=True)\n",
    "        attention_output = attention_layer([combined, combined], return_attention_scores=False)\n",
    "        context_vector = GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "        dense = Dense(units=hp.Int('dense_units', min_value=142, max_value=152, step=1), activation='relu')(context_vector)\n",
    "        dropout = Dropout(0.1)(dense) #Dropout(hp.Float('dropout_rate', min_value=0.0, max_value=0.2, step=0.1))\n",
    "        outdata = Dense(self.num_classes, activation='sigmoid')(dropout)\n",
    "        output = (outdata)\n",
    "        model = Model(inputs=sequence_input, outputs=output)\n",
    "        \n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# Note: When actually using this model for training in tuner, make sure to remove attention_weights from outputs.\n",
    "# Hyperparameters and settings\n",
    "input_length = 20\n",
    "num_classes = 1\n",
    "parallel_blocks = 2\n",
    "log_dir = 'logs/fit/' + datetime.now().strftime(\"%d-%m-%Y %H-%M-%S\")\n",
    "\n",
    "# Use this for training (without attention weights in outputs)\n",
    "hypermodel_for_training = TextClassifierHyperModel(input_length, embeddings_GloVe, num_classes, parallel_blocks)#, include_attention_weights=False)\n",
    "\n",
    "\n",
    "# # Use this for visualization (with attention weights in outputs)\n",
    "# hypermodel_for_visualization = TextClassifierHyperModel(input_length, embeddings_GloVe, num_classes, parallel_blocks, include_attention_weights=True)\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    hypermodel_for_training,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Set the maximum number of trials (model configurations to test)\n",
    "    executions_per_trial=1,  # Number of models that should be built and fit for each trial\n",
    "    directory=log_dir,\n",
    "    project_name='TextClassification'\n",
    ")\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0,  # No histogram computation, set to 1 or higher to compute histograms every '1' epoch or specified frequency\n",
    "    update_freq='epoch'  # Log metrics and histograms every epoch (default), not every batch\n",
    ")\n",
    "\n",
    "# Setup the checkpoint location\n",
    "checkpoint_filepath = '/kaggle/working/best_model.h5'\n",
    "\n",
    "# Create a ModelCheckpoint callback\n",
    "model_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "\n",
    "# Assume X_train, y_train, X_val, y_val are defined\n",
    "tuner.search(X_train, y_train, \n",
    "             epochs=5, \n",
    "             validation_data=(X_test, y_test),\n",
    "             callbacks=[tensorboard_callback, model_callback])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model\\assets\n"
     ]
    }
   ],
   "source": [
    "best_model.save(r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\git_NLP\\Tekstanalyse\\models\\Beast_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 20)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 20, 100)              2223500   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 20, 31)               3131      ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 20, 114)              3648      ['conv1d[0][0]']              \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 20, 114)              3648      ['conv1d_1[0][0]']            \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 20, 216)              192672    ['time_distributed[0][0]']    \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirecti  (None, 20, 216)              192672    ['time_distributed_1[0][0]']  \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 20, 660)              0         ['time_distributed[0][0]',    \n",
      "                                                                     'time_distributed_1[0][0]',  \n",
      "                                                                     'bidirectional[0][0]',       \n",
      "                                                                     'bidirectional_1[0][0]']     \n",
      "                                                                                                  \n",
      " additive_attention (Additi  (None, 20, 660)              660       ['concatenate[0][0]',         \n",
      " veAttention)                                                        'concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 660)                  0         ['additive_attention[0][0]']  \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 150)                  99150     ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 150)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1)                    151       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2722363 (10.38 MB)\n",
      "Trainable params: 498863 (1.90 MB)\n",
      "Non-trainable params: 2223500 (8.48 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits=5\n",
    "\n",
    "KF = KFold(n_splits=n_splits, shuffle=True, random_state=42) # Example: 5-fold cross-validation\n",
    "\n",
    "# Prepare arrays to store results for each fold\n",
    "fold_no = 1\n",
    "loss_per_fold = []\n",
    "acc_per_fold = []\n",
    "\n",
    "for train, test in KF.split(X_train_LSTM, y_train_LSTM):\n",
    "    # Create a fresh model for each fold\n",
    "\n",
    "    # Fit the model\n",
    "    best_model.fit(X_train_LSTM[train], y_train_LSTM[train],\n",
    "                        epochs=n_splits,  # Adjust based on your needs\n",
    "                        batch_size=64,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)],\n",
    "                        validation_data=(X_train_LSTM[test], y_train_LSTM[test]),\n",
    "                        verbose=1)  # You can set verbose to 0 to reduce logs\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = best_model.evaluate(X_train_LSTM[test], y_train_LSTM[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {best_model.metrics_names[0]} of {scores[0]}; {best_model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    loss_per_fold.append(scores[0])\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    fold_no += 1\n",
    "\n",
    "# Print average scores\n",
    "print(f'Average scores for all folds:\\n> Loss: {np.mean(loss_per_fold)}; Accuracy: {np.mean(acc_per_fold)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
