{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as PDF\n",
    "import docx\n",
    "\n",
    "class PDFReader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def extract_text(self):\n",
    "        \"\"\"Extracts text from the entire PDF.\"\"\"\n",
    "        with open(self.file_path, 'rb') as pdfFileObj:\n",
    "            pdfReader = PDF.PdfReader(pdfFileObj)\n",
    "            text = \"\"\n",
    "            for pageNumber in range(len(pdfReader.pages)):\n",
    "                page = pdfReader.pages[pageNumber]\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            return text\n",
    "\n",
    "class ContentFilter:\n",
    "    def __init__(self, keywords):\n",
    "        self.keywords = keywords\n",
    "\n",
    "    def filter_text(self, text):\n",
    "        \"\"\"Filters text to include only paragraphs containing specified keywords.\"\"\"\n",
    "        paragraphs = text.split('\\n')\n",
    "        filtered_paragraphs = [para for para in paragraphs if any(keyword.lower() in para.lower() for keyword in self.keywords)]\n",
    "        return \"\\n\".join(filtered_paragraphs)\n",
    "\n",
    "class DocxWriter:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.doc = docx.Document()\n",
    "\n",
    "    def write_text(self, text):\n",
    "        \"\"\"Writes text to the .docx document, splitting into paragraphs.\"\"\"\n",
    "        paragraphs = text.split('\\n')\n",
    "        for paragraph in paragraphs:\n",
    "            self.doc.add_paragraph(paragraph)\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Saves the .docx document to the specified path.\"\"\"\n",
    "        self.doc.save(self.file_path)\n",
    "\n",
    "# Usage\n",
    "pdf_path = r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\Prosjekt_tekstanalyse\\git_NLP\\Tekstanalyse\\PDF_Reader\\PDF_filer\\AIX_General.pdf'\n",
    "docx_path = r'C:\\Users\\bugat\\Prosjekter\\Tekstanalyse\\Prosjekt_tekstanalyse\\git_NLP\\Tekstanalyse\\PDF_Reader\\PDF_filer\\AIX_General.docx'\n",
    "keywords = ['AI', 'deep learning']\n",
    "\n",
    "reader = PDFReader(pdf_path)\n",
    "extracted_text = reader.extract_text()\n",
    "\n",
    "filter = ContentFilter(keywords)\n",
    "filtered_text = filter.filter_text(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'these few works is DeepRED  algorithm  [257] , which extends  the de- \\n[257] J.R. Zilke , E.L. Mencía , F. Janssen , Deepred–rule  extraction  from deep neural '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_red = ContentFilter(['Deepred', 'DeepRED', 'deepred', 'deep red'])\n",
    "deep_red = filter_red.filter_text(extracted_text)\n",
    "deep_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = DocxWriter(docx_path)\n",
    "writer.write_text(filtered_text)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paragraphs: 909\n",
      "1: Contents  lists available  at ScienceDirect  \n",
      "2: Explainable  Artiﬁcial  Intelligence  (XAI):  Concepts,  taxonomies,  \n",
      "3: opportunities  and challenges  toward  responsible  AI \n",
      "4: a TECNALIA,  Derio 48160, Spain \n",
      "5: b ENSTA, Institute Polytechnique  Paris and INRIA Flowers Team, Palaiseau,  France \n",
      "6: c University  of the Basque Country (UPV/EHU),  Bilbao 48013, Spain \n",
      "7: d Basque Center for Applied Mathematics  (BCAM), Bilbao 48009, Bizkaia, Spain \n",
      "8: g DaSCI Andalusian  Institute of Data Science and Computational  Intelligence,  University  of Granada,  Granada 18071, Spain \n",
      "9: h Telefonica,  Madrid 28050, Spain \n",
      "10: Explainable  Artiﬁcial Intelligence  \n",
      "11: Deep Learning \n",
      "12: Fairness \n",
      "13: In the last few years, Artiﬁcial  Intelligence  (AI) has achieved  a notable momentum  that, if harnessed  appropriately,  \n",
      "14: Machine  Learning,  the entire community  stands in front of the barrier of explainability,  an inherent  problem  of \n",
      "15: in the last hype of AI (namely,  expert systems and rule based models).  Paradigms  underlying  this problem  fall \n",
      "16: within the so-called  eXplainable  AI (XAI) ﬁeld, which is widely acknowledged  as a crucial feature for the practical  \n",
      "17: deployment  of AI models. The overview  presented  in this article examines  the existing literature  and contributions  \n",
      "18: already done in the ﬁeld of XAI, including  a prospect  toward what is yet to be reached.  For this purpose  we \n",
      "19: summarize  previous  eﬀorts made to deﬁne explainability  in Machine  Learning,  establishing  a novel deﬁnition  of \n",
      "20: explainable  Machine  Learning  that covers such prior conceptual  propositions  with a major focus on the audience  \n",
      "21: for which the explainability  is sought. Departing  from this deﬁnition,  we propose and discuss about a taxonomy  \n",
      "22: of recent contributions  related to the explainability  of diﬀerent  Machine  Learning  models, including  those aimed \n",
      "23: at explaining  Deep Learning  methods  for which a second dedicated  taxonomy  is built and examined  in detail. \n",
      "24: This critical literature  analysis  serves as the motivating  background  for a series of challenges  faced by XAI, \n",
      "25: such as the interesting  crossroads  of data fusion and explainability.  Our prospects  lead toward the concept of \n",
      "26: Responsible  Artiﬁcial  Intelligence  , namely,  a methodology  for the large-scale  implementation  of AI methods  in real \n",
      "27: organizations  with fairness,  model explainability  and accountability  at its core. Our ultimate  goal is to provide \n",
      "28: newcomers  to the ﬁeld of XAI with a thorough  taxonomy  that can serve as reference  material  in order to stimulate  \n",
      "29: beneﬁts of AI in their activity sectors, without any prior bias for its lack of interpretability.  \n",
      "30: Artiﬁcial  Intelligence  (AI) lies at the core of many activity  sectors \n",
      "31: of AI trace back to several decades  ago, there is a clear consensus  on \n",
      "32: virtue of these capabilities  that AI methods  are achieving  unprecedented  \n",
      "33: ∗ Corresponding  author at TECNALIA.  P. Tecnologico,  Ed. 700. 48170 Derio (Bizkaia),  Spain. \n",
      "34: E-mail address: javier.delser@tecnalia.com  (J. Del Ser). levels of performance  when learning  to solve increasingly  complex  com- \n",
      "35: human society [2] . The sophistication  of AI-powered  systems  has lately \n",
      "36: sions are furnished  by AI methods  [3] . \n",
      "37: Available  online 26 December  2019 \n",
      "38: Fig. 1. Evolution  of the number of total publications  whose title, abstract  and/or keywords  refer to the ﬁeld of XAI during the last years. Data retrieved  from Scopus®\n",
      "39: (December  10th, 2019) by using the search terms indicated  in the legend when querying  this database.  It is interesting  to note the latent need for interpretable  AI \n",
      "40: to explain AI models has permeated  throughout  the research  community.  \n",
      "41: While the very ﬁrst AI systems  were easily interpretable,  the last \n",
      "42: Neural Networks  (DNNs).  The empirical  success  of Deep Learning  (DL) \n",
      "43: mand for transparency  is increasing  from the various  stakeholders  in AI \n",
      "44: legitimate,  or that simply do not allow obtaining  detailed  explanations  \n",
      "45: ing demand  for ethical AI [3] . It is customary  to think that by focusing  \n",
      "46: detect, and consequently,  correct from bias in the training  dataset.  \n",
      "47: tion of AI systems,  eXplainable  AI (XAI) [7] proposes  creating  a suite of \n",
      "48: ML techniques  that 1) produce  more explainable  models while main- \n",
      "49: taining a high level of learning  performance  (e.g., prediction  accuracy),  \n",
      "50: manage  the emerging  generation  of artiﬁcially  intelligent  partners.  XAI \n",
      "51: Fig. 1 displays  the rising trend of contributions  on XAI and related \n",
      "52: surveys  [8,10,13–17]  summarize  the upsurge  of activity  in XAI across \n",
      "53: sectors and disciplines,  this overview  aims to cover the creation  of a \n",
      "54: scrutiny  and understanding  of the ﬁeld of XAI methods.  Furthermore,  we \n",
      "55: pose intriguing  thoughts  around the explainability  of AI models in data \n",
      "56: Artiﬁcial  Intelligence,  term by which we refer to a series of AI princi- \n",
      "57: ples to be necessarily  met when deploying  AI in real applications.  As \n",
      "58: we will later show in detail, model explainability  is among the most \n",
      "59: 1. Grounded  on a ﬁrst elaboration  of concepts  and terms used in XAI- \n",
      "60: related research,  we propose  a novel deﬁnition  of explainability  that \n",
      "61: plaining  a ML model. We also elaborate  on the diverse  purposes  \n",
      "62: sought when using XAI techniques,  from trustworthiness  to privacy  \n",
      "63: awareness,  which round up the claimed  importance  of purpose  and \n",
      "64: targeted  audience  in model explainability.  \n",
      "65: post-hoc  explainability,  namely,  the explanation  of ML models that \n",
      "66: Fig. 2. Diagram  showing  the diﬀerent  purposes  of explainability  in ML models sought by diﬀerent  audience  proﬁles.  Two goals occur to prevail across them: need \n",
      "67: 3. We thoroughly  analyze  the literature  on XAI and related concepts  \n",
      "68: the explainability  of ML models using the previously  made distinc-  \n",
      "69: tion between  transparency  and post-hoc  explainability,  including  \n",
      "70: shallow  ) learning  models.  The second taxonomy  deals with XAI meth- \n",
      "71: ods suited for the explanation  of Deep Learning  models,  using clas- \n",
      "72: 4. We enumerate  a series of challenges  of XAI that still remain insuf- \n",
      "73: around the concepts  and metrics  to evaluate  the explainability  of ML \n",
      "74: our prospects  toward the implications  of XAI techniques  in regards  \n",
      "75: and other areas intersecting  with explainability.  \n",
      "76: poses the systematic  adoption  of several AI principles  for AI models \n",
      "77: to be of practical  use. In addition  to explainability,  the guidelines  \n",
      "78: behind Responsible  AI establish  that fairness,  accountability  and pri- \n",
      "79: vacy should also be considered  when implementing  AI models in real \n",
      "80: 6. Since Responsible  AI blends together  model explainability  and pri- \n",
      "81: beneﬁts  and risks of XAI techniques  in scenarios  dealing  with sen- \n",
      "82: and governance  demands  more eﬀorts to assess the role of XAI in \n",
      "83: of XAI in terms of privacy  and security  under diﬀerent  data fusion \n",
      "84: The remainder  of this overview  is structured  as follows:  ﬁrst, \n",
      "85: and concepts  revolving  around explainability  and interpretability  in AI, \n",
      "86: ML models from the XAI perspective.  Sections  3 and 4 proceed  by re- \n",
      "87: viewing  recent ﬁndings  on XAI for ML models (on transparent  models \n",
      "88: and post-hoc  techniques  respectively)  that comprise  the main division  in \n",
      "89: the aforementioned  taxonomy.  We also include  a review on hybrid ap- proaches  among the two, to attain XAI. Beneﬁts  and caveats  of the syn- \n",
      "90: outlook  aimed at engaging  the community  around this vibrant  research  \n",
      "91: 2. Explainability:  What, why, what for and how? \n",
      "92: establish  a common  point of understanding  on what the term explain-  \n",
      "93: ability stands for in the context  of AI and, more speciﬁcally,  ML. This \n",
      "94: argue why explainability  is an important  issue in AI and ML (why? what \n",
      "95: for?) and to introduce  the general  classiﬁcation  of XAI approaches  that \n",
      "96: is the interchangeable  misuse of interpretability  and explainability  in \n",
      "97: explainability  can be viewed  as an active characteristic  of a model, de- \n",
      "98: clarifying  or detailing  its internal  functions.  \n",
      "99: ethical AI and XAI communities.  \n",
      "100: how the model works –without  any need for explaining  its internal  \n",
      "101: •Interpretability  : It is deﬁned  as the ability to explain  or to provide  \n",
      "102: •Explainability  : Explainability  is associated  with the notion of expla- \n",
      "103: essential  concept  in XAI. Both transparency  and interpretability  are \n",
      "104: understand  the knowledge  contained  in the model. All in all, under- \n",
      "105: understandability.  This is the reason why the deﬁnition  of XAI given \n",
      "106: cept of audience  the cornerstone  of XAI, as we next elaborate  in further \n",
      "107: detail. \n",
      "108: AI. It appears  from the literature  that there is not yet a common  point of \n",
      "109: understanding  on what interpretability  or explainability  are. However,  \n",
      "110: many contributions  claim the achievement  of interpretable  models and \n",
      "111: techniques  that empower  explainability.  \n",
      "112: Explainable  Artiﬁcial  Intelligence  (XAI) given by D. Gunning  in [7] : \n",
      "113: “XAI will create a suite of machine  learning  techniques  that enables hu- \n",
      "114: sider other purposes  motivating  the need for interpretable  AI models,  \n",
      "115: such as causality,  transferability,  informativeness,  fairness  and conﬁ- \n",
      "116: nition of explainability  in AI still slips from our ﬁngers.  A broader  re- \n",
      "117: formulation  of this deﬁnition  (e.g. “An explainable  Artiﬁcial  Intelligence  \n",
      "118: is one that produces  explanations  about its functioning  ”) would fail to fully \n",
      "119: an explanation  is “the details or reasons that someone  gives to make some- \n",
      "120: this can be rephrased  as: “the details or reasons a model gives to make its \n",
      "121: ambiguities  can be pointed  out. First, the details or the reasons  used \n",
      "122: to explain,  are completely  dependent  of the audience  to which they are \n",
      "123: the explainability  of the model on the audience.  To this end, a reworked  \n",
      "124: Given a certain audience,  explainability  refers to the details and reasons \n",
      "125: Since explaining,  as argumenting,  may involve  weighting,  comparing  \n",
      "126: arguments  [28] , explainability  might convey us into the realm of cogni- \n",
      "127: ternals of a model can be explained  could be tackled  objectively.  Any \n",
      "128: should be considered  as an XAI approach.  How big this leap is in terms \n",
      "129: of complexity  or simplicity  will correspond  to how explainable  the re- \n",
      "130: sulting model is. An underlying  problem  that remains  unsolved  is that \n",
      "131: the interpretability  gain provided  by such XAI approaches  may not be \n",
      "132: straightforward  to quantify:  for instance,  a model simpliﬁcation  can be \n",
      "133: tiﬁcation  of the improvements  gained in terms of interpretability.  The \n",
      "134: derivation  of general  metrics  to assess the quality of XAI approaches  re- \n",
      "135: main as an open challenge  that should be under the spotlight  of the ﬁeld \n",
      "136: Explainability  is linked to post-hoc  explainability  since it covers the \n",
      "137: techniques  used to convert  a non-interpretable  model into a explain-  \n",
      "138: able one. In the remaining  of this manuscript,  explainability  will be \n",
      "139: considered  as the main design objective,  since it represents  a broader  \n",
      "140: concept.  A model can be explained,  but the interpretability  of the \n",
      "141: Bearing  these observations  in mind, explainable  AI can be deﬁned  as \n",
      "142: Given an audience,  an explainable  Artiﬁcial  Intelligence  is one that pro- \n",
      "143: duces details or reasons to make its functioning  clear or easy to under- \n",
      "144: targeted  by XAI techniques  for the model at hand reverts on diﬀerent  \n",
      "145: Goals pursued  in the reviewed  literature  toward reaching  explainability,  and their main target audience.  \n",
      "146: XAI Goal Main target audience  ( Fig. 2 ) References  \n",
      "147: Trustworthiness  Domain experts, users of the model affected by decisions  [5,10,24,32–37]  \n",
      "148: Causality  Domain experts, managers  and executive  board members,  \n",
      "149: Transferability  Domain experts, data scientists  [5,21,26,30,32,37–39,44–85]  \n",
      "150: Conﬁdence  Domain experts, developers,  managers,  regulatory  entities/agencies  [5,35,45,46,48,54,61,72,88,89,96,108,117,119,155]  \n",
      "151: Fairness Users affected by model decisions,  regulatory  entities/agencies  [5,24,35,45,47,99–101,120,121,128,156–158]  \n",
      "152: Interactivity  Domain experts, users affected by model decisions  [37,50,59,65,67,74,86,124]  \n",
      "153: As stated in the introduction,  explainability  is one of the main barri- \n",
      "154: ers AI is facing nowadays  in regards  to its practical  implementation.  The \n",
      "155: inability  to explain  or to fully understand  the reasons  by which state- \n",
      "156: The second axis is that of knowledge.  AI has helped research  across \n",
      "157: able data has largely beneﬁted  from the adoption  of AI and ML tech- \n",
      "158: though for certain disciplines  this might be the fair case, science  and \n",
      "159: goals motivating  the search for explainable  AI models.  \n",
      "160: The research  activity  around XAI has so far exposed  diﬀerent  goals \n",
      "161: to draw from the achievement  of an explainable  model. Almost none \n",
      "162: scribe what an explainable  model should compel.  However,  all these \n",
      "163: exercise  of ML explainability  is performed.  Unfortunately,  scarce con- \n",
      "164: these XAI goals, so as to settle a ﬁrst classiﬁcation  criteria  for the full \n",
      "165: thiness as the primary  aim of an explainable  AI model [31,32]  . How- \n",
      "166: ever, declaring  a model as explainable  as per its capabilities  of in- \n",
      "167: model explainability.  Trustworthiness  might be considered  as the \n",
      "168: a given problem.  Although  it should most certainly  be a property  \n",
      "169: of any explainable  model, it does not imply that every trustwor-  \n",
      "170: thy model can be considered  explainable  on its own, nor is trust- \n",
      "171: worthiness  a property  easy to quantify.  Trust might be far from being the only purpose  of an explainable  model since the relation  \n",
      "172: pose for achieving  explainability.  However,  as seen in Table 1 , they \n",
      "173: to XAI. \n",
      "174: •Causality:  Another  common  goal for explainability  is that of ﬁnding  \n",
      "175: causality  among data variables.  Several  authors  argue that explain-  \n",
      "176: involves  correlation,  so an explainable  ML model could validate  the \n",
      "177: intuition  of possible  causal relationships  within the available  data. \n",
      "178: Again, Table 1 reveals that causality  is not among the most impor- \n",
      "179: •Transferability:  Models  are always bounded  by constraints  that \n",
      "180: should allow for their seamless  transferability.  This is the main rea- \n",
      "181: son why a training-testing  approach  is used when dealing  with ML \n",
      "182: problems  [162,163]  . Explainability  is also an advocate  for transfer-  \n",
      "183: explainable  model, but again, not every transferable  model should \n",
      "184: be considered  as explainable.  As observed  in Table 1 , the amount  of \n",
      "185: papers stating that the ability of rendering  a model explainable  is \n",
      "186: explainability.  \n",
      "187: pitfalls.  For this purpose,  explainable  ML models should give infor- \n",
      "188: However,  other targets for explainability  may include a given example,  the output classes or the dataset itself. \n",
      "189: consider  explaining  the antecedent.  This is the most used argument  \n",
      "190: reaching  explainable  models.  \n",
      "191: is expected.  The methods  to maintain  conﬁdence  under control are \n",
      "192: is a must-have  when drawing  interpretations  from a certain model. \n",
      "193: are not stable. Hence, an explainable  model should contain  informa-  \n",
      "194: •Fairness:  From a social standpoint,  explainability  can be considered  \n",
      "195: as the capacity  to reach and guarantee  fairness  in ML models.  In a \n",
      "196: certain literature  strand, an explainable  ML model suggests  a clear \n",
      "197: visualization  of the relations  aﬀecting  a result, allowing  for a fairness  \n",
      "198: objective  of XAI is highlighting  bias in the data a model was exposed  \n",
      "199: in ﬁelds that involve  human lives, hence explainability  should be \n",
      "200: considered  as a bridge to avoid the unfair or unethical  use of algo- \n",
      "201: for explainability  as the property  that allows end users to get more \n",
      "202: involved  in the process  of improving  and developing  a certain ML \n",
      "203: model [37,86]  . It seems clear that explainable  models will ease the \n",
      "204: an explainable  ML model. Once again, this goal is related to ﬁelds \n",
      "205: the byproducts  enabled  by explainability  in ML models is its ability \n",
      "206: may entail a privacy  breach.  Contrarily,  the ability to explain  the \n",
      "207: inner relations  of a trained  model by non-authorized  third parties \n",
      "208: to its criticality  in sectors where XAI is foreseen  to play a crucial \n",
      "209: scope of the reviewed  papers.  All these goals are clearly under the sur- face of the concept  of explainability  introduced  before in this section.  \n",
      "210: To round up this prior analysis  on the concept  of explainability,  the last \n",
      "211: address  explainability  in ML models.  \n",
      "212: pretable  by design, and those that can be explained  by means of exter- \n",
      "213: nal XAI techniques.  This duality could also be regarded  as the diﬀerence  \n",
      "214: post-hoc  explainability.  This same duality also appears  in the paper pre- \n",
      "215: methods  to solve the transparent  box design problem  against  the prob- \n",
      "216: lem of explaining  the black-box  problem.  This work, further extends  \n",
      "217: rules along with their speciﬁc  output to aid in the understanding  process  \n",
      "218: terms of the domain  in which they are interpretable,  namely,  algorith-  \n",
      "219: next in connection  to Fig. 3 , each of these classes contains  its prede- \n",
      "220: dominant  place in this class. This being said, simple but extensive  \n",
      "221: within.  This aspect aligns with the claim that sparse linear models \n",
      "222: means of text and visualizations  [32] . Again, endowing  a decompos-  \n",
      "223: self-contained  enough  for a human to think and reason about it as a \n",
      "224: •Decomposability  stands for the ability to explain  each of the parts of a \n",
      "225: ability to understand,  interpret  or explain  the behavior  of a model. \n",
      "226: premise).  The added constraint  for an algorithmically  transparent  \n",
      "227: tic optimization  (e.g. through  stochastic  gradient  descent).  The main \n",
      "228: constraint  for algorithmically  transparent  models is that the model \n",
      "229: 2.5.2. Post-hoc  explainability  techniques  for machine  learning  models \n",
      "230: Post-hoc  explainability  targets models that are not readily inter- \n",
      "231: of the most common  ways humans  explain  systems  and processes  by \n",
      "232: nique. Overall,  post-hoc  explainability  techniques  are divided  ﬁrst by \n",
      "233: •Text explanations  deal with the problem  of bringing  explainability  for \n",
      "234: explaining  the results from the model [169] . Text explanations  also \n",
      "235: tioning  of the model. These symbols  may portrait  the rationale  of \n",
      "236: •Visual explanation  techniques  for post-hoc  explainability  aim at vi- \n",
      "237: in the model to users not acquainted  to ML modeling.  •Local explanations  tackle explainability  by segmenting  the solution  \n",
      "238: these only explain  part of the whole system’s  functioning.  \n",
      "239: relate to the result generated  by a certain model, enabling  to get a \n",
      "240: behave when attempting  to explain  a given process,  explanations  \n",
      "241: by example  are mainly centered  in extracting  representative  exam- \n",
      "242: which a whole new system is rebuilt based on the trained  model to be \n",
      "243: explained.  This new, simpliﬁed  model usually  attempts  at optimiz-  \n",
      "244: •Finally,  feature relevance  explanation  methods  for post-hoc  explain-  \n",
      "245: an indirect  method  to explain  a model. \n",
      "246: when reviewing  speciﬁc/agnostic  XAI techniques  for ML models in the \n",
      "247: that maintains  the model under the umbrella  of transparent  methods.  \n",
      "248: However,  as stated in Section  2 , explainability  is linked to a certain \n",
      "249: post-hoc  explainability  techniques  (mainly,  visualization),  particularly  \n",
      "250: when the model is to be explained  to non-expert  audiences.  \n",
      "251: of explaining  the results of the models to non-expert  users. Most authors  \n",
      "252: post-hoc  explainability  approaches  available  for a \n",
      "253: Overall picture of the classiﬁcation  of ML models attending  to their level of explainability.  \n",
      "254: obtain the prediction  of a \n",
      "255: explain the knowledge  \n",
      "256: matches  human naive \n",
      "257: constrained  within human \n",
      "258: adopted  for explaining  these regression  models.  Visualization  tech- \n",
      "259: a model’s  interpretation  might entail, even when its construction  is as \n",
      "260: ear regression  to maintain  decomposability  and simulatability,  its size \n",
      "261: ﬁll every constraint  for transparency.  Decision  trees are hierarchical  \n",
      "262: fall out of the ﬁelds of computation  and AI (even information  technolo-  \n",
      "263: portance.  Tree ensembles  aim at overcoming  such a poor performance  \n",
      "264: ent subsets of training  data. Unfortunately,  the combination  of deci- \n",
      "265: of post-hoc  explainability  techniques  as the ones reviewed  later in the \n",
      "266: In terms of model explainability,  it is important  to observe  that pre- \n",
      "267: similarity  between  examples,  which can be tailored  depending  on the \n",
      "268: being simple to explain,  the ability to inspect the reasons  by which \n",
      "269: over imprecise  domains.  Fuzzy systems  improve  two main axis relevant  \n",
      "270: rule systems  in contexts  with certain degrees  of uncertainty.  Rule based \n",
      "271: plain complex  models by generating  rules that explain  their predictions  \n",
      "272: intepretability.  Similarly,  the speciﬁcity  of the rules plays also against  \n",
      "273: classical  rules to fuzzy rules is to relax the constraints  of rule sizes, since \n",
      "274: makes them very suitable  to understand  and explain  other models.  If a \n",
      "275: certain threshold  of coverage  is acquired,  a rule wrapper  can be thought  \n",
      "276: to contain  enough  information  about a model to explain  its behavior  to a \n",
      "277: one common  factor: understandability.  The main driver for conducting  \n",
      "278: in data. This is why GAMs have been accepted  in certain communities  \n",
      "279: Once again, Bayesian  models fall below the ceiling of Transparent  \n",
      "280: algorithmically  transparent  . However,  it is worth noting that under certain \n",
      "281: to explain  other models,  such as averaging  tree ensembles  [208] . \n",
      "282: 4. Post-hoc  explainability  techniques  for machile  learning  \n",
      "283: models:  Taxonomy,  shallow  models  and deep learning  \n",
      "284: to the model to explain  its decisions.  This is the purpose  of post-hoc  \n",
      "285: explainability  techniques  (also referred  to as post-modeling  explainabil-  \n",
      "286: ity), which aim at communicating  understandable  information  about \n",
      "287: proaches  for post-hoc  explainability,  discriminating  among 1) those that \n",
      "288: tiﬁed around post-hoc  explainability  for diﬀerent  ML models,  which are \n",
      "289: •Model-agnostic  techniques  for post-hoc  explainability  ( Section  4.1 ), \n",
      "290: •Post-hoc  explainability  that are tailored  or speciﬁcally  designed  to \n",
      "291: explain  certain ML models.  We divide our literature  analysis  into \n",
      "292: two main branches:  contributions  dealing  with post-hoc  explain-  \n",
      "293: units ( Section  4.2 ); and techniques  devised  for deep learning  models,  \n",
      "294: nation of Deep Learning  models.  To this end we focus on partic- \n",
      "295: 4.1. Model-agnostic  techniques  for post-hoc  explainability  \n",
      "296: Model-agnostic  techniques  for post-hoc  explainability  are designed  \n",
      "297: times, simpliﬁed  models are only representative  of certain sections  \n",
      "298: the predictions  of an opaque  model to explain  it. These contributions  \n",
      "299: G-REX has been extended  to also account  for model explainability  \n",
      "300: main ideas are exposed:  a method  for model distillation  and com- \n",
      "301: to check if the auditing  data is missing  key features  it was trained  \n",
      "302: porally coincides  with the most recent literature  on XAI, including  \n",
      "303: this post-hoc  explainability  approach  is envisaged  to continue  play- \n",
      "304: ing a central role on XAI. \n",
      "305: •Feature relevance  explanation  techniques  aim to describe  the function-  \n",
      "306: model to be explained.  An amalgam  of propositions  are found within \n",
      "307: Fig. 6. Taxonomy  of the reviewed  literature  and trends identiﬁed  for explainability  techniques  related to diﬀerent  ML models. References  boxed in blue, green \n",
      "308: and red correspond  to XAI techniques  using image, text or tabular data, respectively.  In order to build this taxonomy,  the literature  has been analyzed  in depth to \n",
      "309: discriminate  whether  a post-hoc  technique  can be seamlessly  applied to any ML model, even if, e.g., explicitly  mentions  Deep Learning  in its title and/or abstract.  \n",
      "310: of a classiﬁer  trained  with this subset of features  cannot be distin- \n",
      "311: model’s  prediction  back to the training  data, by only requiring  an \n",
      "312: to its explainability  [236,237]  . Compared  to those attempting  ex- \n",
      "313: found tackling  explainability  by means of feature relevance  tech- \n",
      "314: relevance  has also become  a vibrant  subject study in the current  XAI \n",
      "315: mon in the ﬁeld of model-agnostic  techniques  for post-hoc  explain-  \n",
      "316: rule extraction  techniques  prevail in model-agnostic  contributions  un- \n",
      "317: der the umbrella  of post-hoc  explainability.  This could have been intu- \n",
      "318: explainability  wrappers  anticipated  in Section  3.4 , and the complexity  \n",
      "319: dence of the feature relevance  method  on the model being explained.  \n",
      "320: the trained  model (e.g. its structure,  operations,  etc) are tightly linked \n",
      "321: to the speciﬁc  model to be explained.  \n",
      "322: 4.2. Post-hoc  explainability  in shallow  ML models \n",
      "323: Machines,  SVMs) that require  the adoption  of post-hoc  explainability  \n",
      "324: techniques  for explaining  their decisions.  \n",
      "325: combine  diﬀerent  trees to obtain an aggregated  prediction/regression.  \n",
      "326: While it results to be eﬀective  against  overﬁtting,  the combination  of \n",
      "327: from post-hoc  explainability  techniques.  For tree ensembles,  techniques  \n",
      "328: tree ensembles  while maintaining  part of the accuracy  accounted  for \n",
      "329: the added complexity.  The author from [119] poses the idea of training  \n",
      "330: not that many techniques  to board explainability  in tree ensembles  by \n",
      "331: crease Error) of the forest when a certain variable  is randomly  permuted  \n",
      "332: Finally,  a crosswise  technique  among post-hoc  explainability,  [240] pro- \n",
      "333: ensembles  again, simpliﬁcation  and feature relevance  techniques  seem \n",
      "334: strategies,  scarce activity  has been recently  noted around the explain-  \n",
      "335: interesting  studies on the explainability  of ensemble  techniques  include  \n",
      "336: much opaquer  structure.  Many implementations  of post-hoc  explainabil-  \n",
      "337: gin) to the nearest  training-data  point of any class, since in general,  the \n",
      "338: Section  2 , post-hoc  explainability  applied  to SVMs covers explanation  \n",
      "339: trained  model. This is the approach  of [93] , which proposes  a method  \n",
      "340: that extracts  rules directly  from the support  vectors  of a trained  SVM us- \n",
      "341: tors of a trained  model. The work in [94] generates  fuzzy rules instead  \n",
      "342: of authors  considered  adding the actual training  data as a component  \n",
      "343: formulate  the rule extraction  problem  as a multi-constrained  optimiza-  \n",
      "344: plaining  SVM models when used for concrete  applications.  For instance,  \n",
      "345: [77] presents  an innovative  approach  to visualize  trained  SVM to extract \n",
      "346: weights  of a trained  linear SVM that allows for a much more compre-  \n",
      "347: is speciﬁc  enough  to explain  the multivariate  patterns  shown in neu- \n",
      "348: tems, the latter being adopted  as a post-hoc  technique  to explain  de- \n",
      "349: hoc explainability.  In SVMs, local explanations  have started to take some \n",
      "350: plainability  are dated beyond  2017, which might be due to the progres-  \n",
      "351: 4.3. Explainability  in deep learning  \n",
      "352: creasingly  the most adopted  methods  for explaining  DNNs. This section \n",
      "353: reviews  explainability  studies proposed  for the most used DL models,  \n",
      "354: their questionable  explainability  a common  reason for reluctance.  That \n",
      "355: els. The fact that explainability  is often a must for the model to be of \n",
      "356: practical  value, forced the community  to generate  multiple  explainabil-  \n",
      "357: Some other works use model simpliﬁcation  as a post-hoc  explainabil-  \n",
      "358: is more complex  as the number  of layers increases,  explaining  these \n",
      "359: of current  explainability  methods.  For example,  the authors  in [262] , \n",
      "360: scores. This structure  entails extremely  complex  internal  relations  that \n",
      "361: are very diﬃcult  to explain.  Fortunately,  the road to explainability  for \n",
      "362: skills favors the understanding  of visual data. Existing  works that aim at understanding  what CNNs learn can be \n",
      "363: curacy on several image recognition  benchmarks.  They obtained  a \n",
      "364: each ﬁlter to learn very speciﬁc  object components.  The obtained  acti- \n",
      "365: Fig. 7. Examples  of rendering  for diﬀerent  XAI visualization  techniques  on images. \n",
      "366: of a certain network  [293] . \n",
      "367: sented a three-level  attention  model to perform  a ﬁne-grained  classiﬁ-  \n",
      "368: ﬁlters out non-relevant  patches  to a certain object, and the last attention  \n",
      "369: der to analyse  the visual information  contained  inside the CNN, the \n",
      "370: the CNN internal  representations  and showed  that several layers retain \n",
      "371: paper also examines  the eﬀects of classical  training  techniques  on the \n",
      "372: are arguably  the most adopted  approach  to explainability  in CNNs. Instead  of using one single interpretability  technique,  the framework  \n",
      "373: A completely  diﬀerent  explainability  approach  is proposed  in adver- \n",
      "374: sarial detection.  To understand  model failures  in detecting  adversarial  \n",
      "375: from the representations  of the training  images.  \n",
      "376: As occurs with CNNs in the visual domain,  RNNs have lately been \n",
      "377: network  considered  as a black-box  can be explained  by associ- \n",
      "378: Few contributions  have been made for explaining  RNN models.  \n",
      "379: These studies can be divided  into two groups:  1) Explainability  by un- \n",
      "380: derstanding  what a RNN model has learned  (mainly  via feature relevance  \n",
      "381: methods);  and 2) explainability  by modifying  RNN architectures  to pro- \n",
      "382: dient Boosting  Trees to the trained  LSTM network  under focus. \n",
      "383: of the RNNs, [285] presents  RETAIN  (REverse  Time AttentIoN)  model, \n",
      "384: or constraints  in Knowledge  Bases (KBs) has shown to not only improve  \n",
      "385: explainability  but also performance  with respect to purely data-driven  \n",
      "386: present  in the training  data labels. Other approaches  have shown to be \n",
      "387: models with explainability  by externalizing  other domain  information  \n",
      "388: DNN, that must be supported  by the training  data [264] . \n",
      "389: A diﬀerent  perspective  on hybrid XAI models consists  of enriching  \n",
      "390: can be done by constraining  the neural network  thanks to a semantic  \n",
      "391: explainable  autoencoders  are proposed  [313] . A speciﬁc  transformer  \n",
      "392: relevant  sections.  Transformers  can also help explain  image captions  vi- \n",
      "393: the CBR (in this case a kNN) are paired in order to improve  interpretabil-  \n",
      "394: CBR, in order to retrieve  nearest-neighbor  cases to explain  the DNN’s \n",
      "395: 4.4. Alternative  taxonomy  of post-hoc  explainability  techniques  for deep \n",
      "396: erature  on XAI. While the division  between  model-agnostic  and model- \n",
      "397: only relied on this criteria  to classify  XAI methods.  For instance,  some \n",
      "398: model-agnostic  methods  such as SHAP [224] are widely used to explain  \n",
      "399: DL models.  That is why several XAI methods  can be easily categorized  \n",
      "400: Explaining  a Deep Network  Processing  , as a kind of Linear Proxy Model . \n",
      "401: plaining  the processing  of data by the network,  thus answering  to the \n",
      "402: The second one concerns  methods  explaining  the representation  of data \n",
      "403: the network  contain?  ”. The third approach  concerns  models speciﬁcally  \n",
      "404: structing  XAI taxonomies.  \n",
      "405: Fig. 11. (a) Alternative  Deep Learning  speciﬁc taxonomy  extended  from the categorization  from [13] ; and (b) its connection  to the taxonomy  in Fig. 6 . See References  \n",
      "406: Fig. 11 shows the alternative  Deep Learning  taxonomy  inferred  from \n",
      "407: structure.  This means that depending  of the failure reasons  of a complex  \n",
      "408: model, it would be possible  to pick-up  the right XAI method  according  \n",
      "409: 5. XAI: Opportunities,  challenges  and future research  needs \n",
      "410: addressed  in the ﬁeld of explainability  of ML and data fusion models.  \n",
      "411: them and explore  new research  opportunities  for XAI, identifying  pos- \n",
      "412: Section  5.1 we will stress on the potential  of XAI developments  to \n",
      "413: sensus on what explainability  entails within the AI realm. Reasons  \n",
      "414: for pursuing  explainability  are also assorted  and, under our own \n",
      "415: •Given its notable  prevalence  in the XAI literature,  Section  4.3 and \n",
      "416: 4.4 revolved  on the explainability  of Deep Learning  models,  examin-  \n",
      "417: several challenges  that hold in regards  to the explainability  of this \n",
      "418: connection  to model explainability,  remain insuﬃciently  studied  by \n",
      "419: which enumerates  research  needs and open questions  related to XAI \n",
      "420: within a broader  context:  the need for responsible  AI. \n",
      "421: potential  of XAI techniques  and tools resides. \n",
      "422: strained  within very controlled  physical  problems,  in which all of the \n",
      "423: entails certain complexity,  that the data available  for study is greatly \n",
      "424: the added complexity  of the model will only ﬁght against  the task of \n",
      "425: tion of more sophisticated  methods  for explainability  could invert or \n",
      "426: spired by previous  works [7] , in which XAI shows its power to improve  \n",
      "427: The literature  clearly asks for an uniﬁed concept  of explainability.  In \n",
      "428: the ﬁeld. It should propose  a common  structure  for every XAI system.  \n",
      "429: This paper attempted  a new proposition  of a concept  of explainability  that is built upon that from Gunning  [7] . In that proposition  and the \n",
      "430: following  strokes to complete  it ( Section  2.2 ), explainability  is deﬁned  \n",
      "431: common  ground  and reference  point to sustain a proﬁtable  discussion  in \n",
      "432: this matter.  It is paramount  that the ﬁeld of XAI reaches  an agreement  in \n",
      "433: Another  key feature needed  to relate a certain model to this con- \n",
      "434: deﬁnition  of explainable.  Without  such tool, any claim in this respect \n",
      "435: should express  how well the model performs  in a certain aspect of ex- \n",
      "436: plainability.  Some attempts  have been done recently  around the mea- \n",
      "437: surement  of XAI, as reviewed  thoroughly  in [349,350]  . In general,  XAI \n",
      "438: tal models,  computational  measures  for explainer  ﬁdelity,  explanation  \n",
      "439: rection of evaluating  XAI techniques.  Unfortunately,  conclusions  drawn \n",
      "440: quantiﬁable,  general  XAI metrics  are really needed  to support  the exist- \n",
      "441: whole prior acceptance  of the broader  concept  of explainability,  which \n",
      "442: on the other hand, is one of the aims of the current  work. Nevertheless,  \n",
      "443: performance  of XAI techniques,  as well as comparison  methodologies  \n",
      "444: among XAI approaches  that allow contrasting  them quantitatively  under \n",
      "445: 5.3. Challenges  to achieve explainable  deep learning  \n",
      "446: While many eﬀorts are currently  being made in the area of XAI, there \n",
      "447: are still many challenges  to be faced before being able to obtain explain-  \n",
      "448: ability in DL models.  First, as explained  in Section  2.2 , there is a lack of \n",
      "449: XAI. As an example,  we often see the terms feature importance  and feature \n",
      "450: vations,  attribution,  and other approaches  alike. As XAI is a relatively  \n",
      "451: why, as mentioned  above, a challenge  in XAI is establishing  objective  \n",
      "452: explainable  AI model are highlighted  in [12] : First, explanations  are \n",
      "453: plained  that probabilities  are not as important  as causal links in order \n",
      "454: to translate  the probabilistic  results into qualitative  notions  contain-  \n",
      "455: meaning  that focusing  solely on the main causes of a decision-making  \n",
      "456: eﬃcient,  while they oﬀer a greater explainability  thus respecting  the \n",
      "457: •Being selective  is less straightforward  for connectionist  models than \n",
      "458: As stated in [24] , a truly explainable  model should not leave expla- \n",
      "459: A ﬁnal challenge  XAI methods  for DL need to address  is provid-  \n",
      "460: ties, and to develop  the social right to the (not-yet  available)  right \n",
      "461: 5.4. Explanations  for AI security:  XAI and adversarial  machine  learning  \n",
      "462: Nothing  has been said about conﬁdentiality  concerns  linked to XAI. \n",
      "463: thing secret , in the AI context  many aspects  involved  in a model may \n",
      "464: to make DL models more robust against  intellectual  property  exposure  \n",
      "465: XAI tools capable  of explaining  ML models while keeping  the model’s  \n",
      "466: Ideally,  XAI should be able to explain  the knowledge  within an AI \n",
      "467: However,  the information  revealed  by XAI techniques  can be used both \n",
      "468: to generate  more eﬀective  attacks in adversarial  contexts  aimed at con- \n",
      "469: protect against  private content  exposure  by using such information.  Ad- \n",
      "470: [359] . For the particular  case of DL models,  available  solutions  such as \n",
      "471: diﬀerent  approaches  to harden the model against  them. Other examples  \n",
      "472: evasion  attacks.  There are even available  solutions  for unsupervised  ML, \n",
      "473: While XAI techniques  can be used to furnish more eﬀective  adversar-  \n",
      "474: other generative  models towards  explaining  data-based  decisions.  Once \n",
      "475: trained,  generative  models can generate  instances  of what they have \n",
      "476: adopted  by several recent studies [366,367]  mainly as an attribution  \n",
      "477: method  to relate a particular  output of a Deep Learning  model to their \n",
      "478: 5.5. XAI and output conﬁdence  \n",
      "479: pend on the output of AI models,  such as vehicular  perception  and self- \n",
      "480: yielded  comprehensive  regulatory  eﬀorts aimed at ensuring  that no de- \n",
      "481: risk and uncertainty  of harms derived  from decisions  made on the out- \n",
      "482: the share of epistemic  uncertainty  (namely,  the uncertainty  due to lack \n",
      "483: jection of the model’s  output [370,371]  . To this end, explaining  via XAI \n",
      "484: uncertainty  within the input domain.  \n",
      "485: 5.6. XAI, Rationale  explanation,  and critical data studies \n",
      "486: also to the availability  of information  about the full discourse  around \n",
      "487: this discourse  we ﬁnd also an interesting  space for the adoption  of XAI \n",
      "488: XAI can eﬀectively  ease the process  of explaining  the reasons  why a \n",
      "489: make them appraise  the ethical implications  of their data-based  choices  \n",
      "490: where XAI can signiﬁcantly  boost the exchange  of information  among \n",
      "491: 5.7. XAI And theory-guided  data science \n",
      "492: We envision  an exciting  synergy  between  the XAI realm and Theory-  \n",
      "493: is previously  known.  Similarly,  the training  approach  should not allow \n",
      "494: available  with promising  results.  The studies in [375–382]  were carried \n",
      "495: XAI. All the additions  presented  in [374] push toward techniques  that \n",
      "496: would eventually  render a model explainable,  and furthermore,  knowl- \n",
      "497: tured by a model should be explained  for assessing  its compliance  with \n",
      "498: theoretical  principles  known beforehand.  This, again, opens a magniﬁ-  \n",
      "499: cent window  of opportunity  for XAI. \n",
      "500: 5.8. Guidelines  for ensuring  interpretable  AI models \n",
      "501: nature of the process  of making  an AI-based  model interpretable.  Along \n",
      "502: teracting  with the system to be explained,  from the designers  of the sys- \n",
      "503: guidelines  to implement  and explain  AI systems  have been recently  con- \n",
      "504: suggests  that the incorporation  and consideration  of explainability  in \n",
      "505: practical  AI design and deployment  workﬂows  should comprise  four ma- \n",
      "506: 1. Contextual  factors,  potential  impacts  and domain-speciﬁc  needs \n",
      "507: for which the AI model is built, the complexity  of explanations  that \n",
      "508: a reference  point for the AI system to be deployed  in lieu thereof.  \n",
      "509: considering  explainability  in the development  of an AI system,  the \n",
      "510: decision  of which XAI approach  should be chosen should gauge \n",
      "511: domain-speciﬁc  risks and needs, the available  data resources  and \n",
      "512: existing  domain  knowledge,  and the suitability  of the ML model to \n",
      "513: and domain-speciﬁc  needs) can make transparent  models preferable  \n",
      "514: the application  of post-hoc  XAI techniques.  By contrast,  black-box  \n",
      "515: that ethics-,  fairness-  and safety-related  impacts  should be weighed.  \n",
      "516: AI system should be ensured  by checking  whether  such identiﬁed  \n",
      "517: system with XAI tools that provide  the level of explainability  re- \n",
      "518: quired by the domain  in which it is deployed.  To this end, the third \n",
      "519: guideline  suggests  1) a detailed  articulation,  examination  and eval- \n",
      "520: whether  the coverage  and scope of the available  explanatory  ap- \n",
      "521: proaches  match the requirements  of the domain  and application  \n",
      "522: livery strategy,  including  a detailed  time frame for the execution  of \n",
      "523: measures  of explainability  are intensively  revolving  by considering  \n",
      "524: AI. Methodological  principles  ensure that the purpose  for which explain-  \n",
      "525: relevance  such as no discrimination,  sustainability,  privacy  or account-  \n",
      "526: ability. A challenge  remains  in harnessing  the potential  of XAI to realize \n",
      "527: a Responsible  AI , as we discuss in the next section.  \n",
      "528: 6. Toward  responsible  AI: Principles  of artiﬁcial  intelligence,  \n",
      "529: fairness,  privacy  and data fusion \n",
      "530: published  guidelines  to indicate  how AI should be developed  and used. \n",
      "531: These guidelines  are commonly  referred  to as AI principles  , and they \n",
      "532: tackle issues related to potential  AI threats to both individuals  and to \n",
      "533: and widely recognized  principles  in order to link XAI –which  normally  \n",
      "534: implementation  and use of AI models be sought in practice,  it is our ﬁrm \n",
      "535: claim that XAI does not suﬃce on its own. Other important  principles  of \n",
      "536: Artiﬁcial  Intelligence  such as privacy  and fairness  must be carefully  ad- \n",
      "537: of Responsible  AI, along with the implications  of XAI and data fusion in \n",
      "538: A recent review of some of the main AI principles  published  since \n",
      "539: explainability,  or fairness.  They also consider  the coverage  that the \n",
      "540: •Target audience:  to whom the principles  are aimed. They are nor- \n",
      "541: For instance,  [386] is an illustrative  example  of a document  of AI \n",
      "542: of the most common  principles,  and deals explicitly  with explainability.  \n",
      "543: Here, the authors  propose  ﬁve principles  mainly to guide the develop-  \n",
      "544: ment of AI within their company,  while also indicating  that they could \n",
      "545: The authors  of those principles  aim to develop  AI in a way that \n",
      "546: •The outputs  after using AI systems  should not lead to any kind of \n",
      "547: discrimination  against  individuals  or collectives  in relation  to race, \n",
      "548: while optimizing  the results of an AI system is not only their outputs  \n",
      "549: those groups.  This deﬁnes the principle  of Fair AI . \n",
      "550: person,  and when they are communicating  with an AI system.  Peo- \n",
      "551: by the AI system and for what purpose.  It is crucial to ensure a \n",
      "552: certain level of understanding  about the decisions  taken by an AI \n",
      "553: system.  This can be achieved  through  the usage of XAI techniques.  \n",
      "554: Transparent  and Explainable  AI . \n",
      "555: •AI products  and services  should always be aligned  with the United \n",
      "556: Nation’s  Sustainable  Development  Goals [387] and contribute  to \n",
      "557: them in a positive  and tangible  way. Thus, AI should always gen- \n",
      "558: principle  of Human-centric  AI (also referred  to as AI for Social Good \n",
      "559: •AI systems,  specially  when they are fed by data, should always con- \n",
      "560: principle  is not exclusive  of AI systems  since it is shared with many \n",
      "561: and recommendations  aimed at considering  a wider context  for sci- \n",
      "562: challenges  such as sustainability,  public engagement,  ethics, science  \n",
      "563: ple of Transparent  and Explainable  AI mentioned  previously.  \n",
      "564: Going beyond  the scope of these ﬁve AI principles,  the European  \n",
      "565: worthy  AI [390] through  an assessment  checklist  that can be completed  \n",
      "566: by diﬀerent  proﬁles  related to AI systems  (namely,  product  managers,  \n",
      "567: discrimination  and fairness;  5) societal  and environmental  well-being;  \n",
      "568: 6) accountability.  These principles  are aligned  with the ones detailed  \n",
      "569: including  any type of organization  involved  in the development  of AI. \n",
      "570: It is worth mentioning  that most of these AI principles  guides directly  \n",
      "571: approach  XAI as a key aspect to consider  and include  in AI systems.  In \n",
      "572: that 28 out of the 32 AI principles  guides covered  in the analysis,  explic- \n",
      "573: itly include  XAI as a crucial component.  Thus, the work and scope of this \n",
      "574: AI at a worldwide  level. \n",
      "575: 6.2. Fairness  and accountability  \n",
      "576: pects, beyond  XAI, included  within the diﬀerent  AI principles  guidelines  \n",
      "577: pletely detached  from XAI; in fact, they are intertwined.  This section \n",
      "578: presents  two key components  with a huge relevance  within the AI prin- \n",
      "579: ciples guides, Fairness  and Accountability.  It also highlights  how they \n",
      "580: are connected  to XAI. \n",
      "581: 6.2.1. Fairness  and discrimination  \n",
      "582: tected and unprotected  features  where XAI techniques  ﬁnd their place \n",
      "583: ables amenable  to cause discrimination.  XAI techniques  such as SHAP \n",
      "584: [224] could be used to generate  counterfactual  outcomes  explaining  the \n",
      "585: Recalling  the Fair AI principle  introduced  in the previous  section,  \n",
      "586: [386] reminds  that fairness  is a discipline  that generally  includes  pro- \n",
      "587: unintentionally  create unfair decisions  by considering  sensitive  factors \n",
      "588: unfair decisions  can give rise to discriminatory  issues, either by explic- \n",
      "589: aforementioned  proposals  centered  on fairness  aspects  permit to dis- \n",
      "590: •Individual  fairness:  here, fairness  is analyzed  by modeling  the diﬀer- \n",
      "591: •Group fairness:  it deals with fairness  from the perspective  of all in- \n",
      "592: •Counterfactual  fairness:  it tries to interpret  the causes of bias using, \n",
      "593: •Tainted  data: Errors in the data modelling  deﬁnition,  wrong feature \n",
      "594: to deﬁne when AI is not biased. For supervised  ML, [393] presents  a \n",
      "595: framework  that uses three criteria  to evaluate  group fairness  when there \n",
      "596: fairness.  \n",
      "597: ML model is trained,  looking  to remove  the bias at the ﬁrst step of \n",
      "598: •In-processing:  These techniques  are applied  during the training  pro- \n",
      "599: cess of the ML model. Normally,  they include  Fairness  optimization  \n",
      "600: constraints  along with cost functions  of the ML model. An example  \n",
      "601: is trained.  They are less intrusive  because  they do not modify the \n",
      "602: Even though these references  apparently  address  an AI principle  that \n",
      "603: appears  to be independent  of XAI, the literature  shows that they are in- \n",
      "604: AI principles  that deal with XAI, also talk about fairness  explicitly.  This \n",
      "605: when implementing  Responsible  AI. \n",
      "606: The literature  also exploses  that XAI proposals  can be used for bias \n",
      "607: the bias present  in a model (both for individual  and group fairness).  \n",
      "608: Thus, the fairness  report is shown just like the visual summaries  used \n",
      "609: within XAI. This explainability  approach  eases the understanding  and \n",
      "610: it quantitatively,  indicate  the degree of fairness,  and explain  why a user \n",
      "611: or group would be treated unfairly  with the available  data. Similarly,  \n",
      "612: XAI techniques  such as SHAP [224] could be used to generate  coun- \n",
      "613: terfactual  outcomes  explaining  the decisions  of a ML model when fed \n",
      "614: lations between  protected  and unprotected  features  through  XAI tech- \n",
      "615: Another  example  is [399] , where the authors  propose  a fair-by-  \n",
      "616: a small part of the whole dataset available  (weak supervision).  It ﬁrst \n",
      "617: it generates  rules in an IF/THEN  format that explain  that a data point \n",
      "618: cently pursued  in [401] , showing  that post-hoc  XAI techniques  can forge \n",
      "619: fairer explanations  from truly unfair black-box  models.  Finally,  CERTI- \n",
      "620: FAI (Counterfactual  Explanations  for Robustness,  Transparency,  Inter- \n",
      "621: pretability,  and Fairness  of Artiﬁcial  Intelligence  models)  [402] uses a \n",
      "622: ine fairness  (both at the individual  level and at the group level) at the \n",
      "623: Strongly  linked to the concept  of fairness,  much attention  has been \n",
      "624: with ethical restrictions  that permeate  to the AI modeling  phase [404] . \n",
      "625: Likewise,  certain AI problems  (such as content  recommendation  or infor- \n",
      "626: mation retrieval)  also aim at producing  diverse  recommendations  rather \n",
      "627: secting the internals  of a black-box  model via XAI techniques  can help \n",
      "628: identifying  the capability  of the model to maintain  the input data di- \n",
      "629: sity keeping  capabilities  could be complemented  with XAI techniques  \n",
      "630: from which the model was trained.  Conversely,  XAI could help to dis- \n",
      "631: AI systems.  Performing  the assessment  by both internal  and external  \n",
      "632: auditors,  and making  the reports available,  could contribute  to the \n",
      "633: trustworthiness  of the technology.  When the AI system aﬀects funda- \n",
      "634: porting  actions or decisions  that yield a certain outcome  by the sys- \n",
      "635: to respond  to them. To address  that, the development  of AI systems  \n",
      "636: and use of AI systems.  It is also important  to guarantee  protection  \n",
      "637: for anyone  who raises concerns  about an AI system (e.g., whistle-  \n",
      "638: AI systems  pose. •Trade-oﬀs:  In case any tension  arises due to the implementation  of \n",
      "639: use of the AI system should not proceed  in that form. \n",
      "640: sure trust. Special attention  should be paid to vulnerable  persons  or \n",
      "641: of XAI with accountability.  First, XAI contributes  to auditability  as it \n",
      "642: can help explaining  AI systems  for diﬀerent  proﬁles,  including  regula- \n",
      "643: tory ones. Also, since there is a connection  between  fairness  and XAI as \n",
      "644: stated before, XAI can also contribute  to the minimization  and report of \n",
      "645: exist in almost all domains  of activity  calls for data fusion approaches  \n",
      "646: aimed at exploiting  them simultaneously  toward solving  a learning  task. \n",
      "647: the potential  of data fusion techniques  to enrich the explainability  of \n",
      "648: sible AI, the conﬂuence  between  XAI and data fusion is an uncharted  \n",
      "649: research  area in the current  research  mainstream.  \n",
      "650: ( Fig. 13 .e). Upon local model training,  clients transmit  encrypted  in- \n",
      "651: object as per the information  contained  in the diﬀerent  data sources  \n",
      "652: to diﬀerent  algorithmic  means, from co-training  to co-regularization  \n",
      "653: responsible  AI paradigm  \n",
      "654: AI systems,  specially  when dealing  with multiple  data sources,  need \n",
      "655: •Privacy  and data protection:  they should be guaranteed  in AI systems  \n",
      "656: user could be used in a negative  way against  them (discrimination  \n",
      "657: due to sensitive  features,  unfair treatment...),  it is crucial to ensure \n",
      "658: reach good performance  with AI systems  that are fueled with data, \n",
      "659: like ML. However,  sometimes  the data collected  contains  socially  \n",
      "660: be tackled  before training  any model with the data collected.  Addi- \n",
      "661: is directly  intertwined  with privacy  and with fairness,  regardless  of the \n",
      "662: ing input queries  on the model [356,357]  . An approach  to explain  loss \n",
      "663: direction,  namely,  in ensuring  that XAI methods  do not pose a threat in \n",
      "664: regards  to the privacy  of the data used for training  the ML model under \n",
      "665: the context  of explainability  covered  in this survey.  To begin with, clas- \n",
      "666: no connection  to the ML model, so they have little to do with explain-  \n",
      "667: can be thought  to aim at solving  a data level fusion problem,  yet in a \n",
      "668: In this context,  many techniques  in the ﬁeld of XAI have been pro- \n",
      "669: paves the way to explaining  how data sources  are actually  fused through  \n",
      "670: spatial and/or time domain.  Ultimately,  this gained information  on the \n",
      "671: level contemplates  data under certain constraints  of known form and \n",
      "672: in the possibility  that XAI techniques  could be explanatory  enough  to \n",
      "673: the explained  fusion among protected  and unprotected  features.  \n",
      "674: making  it necessary  to resort to post-hoc  explainability  solutions.  How- \n",
      "675: ever, model fusion may entail other drawbacks  when endowed  with \n",
      "676: powerful  post-hoc  XAI techniques.  Let us imagine  that relationships  of \n",
      "677: tend to) models are fused. These models contain  among others, cell- \n",
      "678: If focused  at knowledge  level fusion, a similar reasoning  holds: XAI \n",
      "679: ability to explain  models could have an impact on the necessity  of \n",
      "680: within ML models.  If so, XAI might enrich knowledge  fusion paradigms,  \n",
      "681: portance  that the knowledge  extracted  from a model by means of XAI \n",
      "682: techniques  can be understood  and extrapolated  to the domain  in which \n",
      "683: of transfer  learning  portrayed  in [425] . Although  XAI is not contem-  \n",
      "684: trained  in certain feature spaces and distributions,  to then be utilized  in \n",
      "685: XAI can pose a threat if the explanations  given about the model can be \n",
      "686: spurs further challenges  in regards  to privacy  and explainability.  The \n",
      "687: Distributed  fusion might be applied  for diﬀerent  reasons,  mainly due to \n",
      "688: environmental  constraints  or due to security  or privacy  issues. The latter \n",
      "689: of an ML model trained  on local data. This rationale  lies at the heart \n",
      "690: This lightens  the training  process  for network-compromised  settings  and \n",
      "691: guarantees  data privacy  [416] . Upon the use of post-hoc  explainability  \n",
      "692: cal context  in which the received  ML model part was trained.  In fact, \n",
      "693: every single hyperparameter  value used for training,  allowing  for poten- \n",
      "694: Data fusion, privacy  and model explainability  are concepts  that have \n",
      "695: 6.4. Implementing  responsible  AI principles  in an organization  \n",
      "696: While increasingly  more organizations  are publishing  AI principles  \n",
      "697: Fig. 14. Summary  of XAI challenges  discussed  in this overview  and its impact on the principles  for Responsible  AI. \n",
      "698: •AI-speciﬁc  principles  that focus on aspects  that are speciﬁc  to AI, \n",
      "699: such as explainability,  fairness  and human agency.  \n",
      "700: •End-to-end  principles  that cover all aspects  involved  in AI, including  \n",
      "701: The EC Guidelines  for Trustworthy  AI are an example  of end-to-end  \n",
      "702: operating  worldwide)  are more AI-speciﬁc  [386] . For example,  safety \n",
      "703: also for AI systems.  The same holds for privacy,  but it is probably  true \n",
      "704: that privacy  in the context  of AI systems  is even more important  than for \n",
      "705: data and most importantly,  because  XAI tools and data fusion techniques  \n",
      "706: When it comes to implement  the AI Principles  into an organization,  it \n",
      "707: is important  to operationalize  the AI-speciﬁc  parts and, at the same time, \n",
      "708: for privacy,  security  and safety. Implementing  AI principles  requires  a \n",
      "709: •AI principles  (already  discussed  earlier),  which set the values and \n",
      "710: •Awareness  and training  about the potential  issues, both technical  \n",
      "711: •A questionnaire  that forces people to think about certain impacts  of \n",
      "712: the AI system ( impact explanation  ). This questionnaire  should give \n",
      "713: concrete  guidance  on what to do if certain undesired  impacts  are \n",
      "714: ing any problems  identiﬁed.  XAI tools and fairness  tools fall in this \n",
      "715: nance: 1) based on committees  that review and approve  AI devel- \n",
      "716: Responsible  AI principles  in companies  should balance  between  two re- quirements:  1) Major cultural  and organizational  changes  needed  to en- \n",
      "717: force such principles  over processes  endowed  with AI functionalities;  \n",
      "718: principles  with the IT assets, policies  and resources  already  available  at \n",
      "719: around the principles  and values of Responsible  AI where we envision  \n",
      "720: that XAI will make its place and create huge impact.  \n",
      "721: This overview  has revolved  around eXplainable  Artiﬁcial  Intelli- \n",
      "722: gence (XAI), which has been identiﬁed  in recent times as an utmost need \n",
      "723: model explainability,  as well as by showing  the diverse  purposes  that \n",
      "724: cent literature  dealing  with explainability,  which has been approached  \n",
      "725: and 2) post-hoc  XAI techniques  devised  to make ML models more in- \n",
      "726: explainability  of Deep Learning  models,  we have inspected  in depth the \n",
      "727: tive taxonomy  that connects  more closely with the speciﬁc  domains  in \n",
      "728: which explainability  can be realized  for Deep Learning  models.  \n",
      "729: in the XAI realm toward the concept  of Responsible  AI, a paradigm  that \n",
      "730: imposes  a series of AI principles  to be met when implementing  AI models \n",
      "731: in practice,  including  fairness,  transparency,  and privacy.  We have also \n",
      "732: discussed  the implications  of adopting  XAI techniques  in the context  of \n",
      "733: data fusion, unveiling  the potential  of XAI to compromise  the privacy  \n",
      "734: of protected  data involved  in the fusion process.  Implications  of XAI in \n",
      "735: fairness  have also been discussed  in detail. This vision of XAI as a core \n",
      "736: concept  to ensure the aforementioned  principles  for Responsible  AI is \n",
      "737: Our reﬂections  about the future of XAI, conveyed  in the discus- \n",
      "738: XAI techniques.  It is our vision that model interpretability  must be ad- \n",
      "739: dressed  jointly with requirements  and constraints  related to data pri- \n",
      "740: vacy, model conﬁdentiality,  fairness  and accountability.  A responsible  \n",
      "741: implementation  and use of AI methods  in organizations  and institutions  \n",
      "742: worldwide  will be only guaranteed  if all these AI principles  are studied  \n",
      "743: received  through  the EMAITEK  and ELKARTEK  programs.  Javier Del \n",
      "744: research  and innovation  programme  AI4EU under grant agreement  \n",
      "745: ble AI: R. Benjamins,  A. Barbado,  D. Sierra, “Responsible  AI by Design ”, to \n",
      "746: appear in the Proceedings  of the Human-Centered  AI: Trustworthiness  \n",
      "747: of AI Models  & Data (HAI) track at AAAI Fall Symposium,  DC, November  \n",
      "748: [2] D.M. West , The future of work: robots, AI, and automation,  Brookings  Institution  \n",
      "749: ing and a “right to explanation  ”, AI Magazine  38 (3) (2017) 50–57 . \n",
      "750: [4] D. Castelvecchi  , Can we open the black box of AI? Nature News 538 (7623) (2016) \n",
      "751: [6] A. Preece, D. Harborne,  D. Braines, R. Tomsett, S. Chakraborty,  Stakeholders  in \n",
      "752: Explainable  AI, 2018. \n",
      "753: [7] D. Gunning , Explainable  artiﬁcial intelligence  (xAI), Technical  Report, Defense Ad- \n",
      "754: [8] E. Tjoa, C. Guan, A survey on explainable  artiﬁcial intelligence  (XAI): Towards \n",
      "755: medical XAI, 2019. \n",
      "756: [9] J. Zhu, A. Liapis, S. Risi, R. Bidarra, G.M. Youngblood,  Explainable  AI for \n",
      "757: [10] F.K. Dos ̃ilovi ć, M. Brc ̃i ć, N. Hlupi ć, Explainable  artiﬁcial intelligence:  A survey, \n",
      "758: [13] L.H. Gilpin, D. Bau, B.Z. Yuan, A. Bajwa, M. Specter, L. Kagal, Explaining  Explana- \n",
      "759: [14] A. Adadi , M. Berrada , Peeking inside the black-box:  A survey on explainable  arti- \n",
      "760: ﬁcial intelligence  (XAI), IEEE Access 6 (2018) 52138–52160  . \n",
      "761: in: IJCAI-17 workshop  on explainable  AI (XAI), 8, 2017, p. 1 . \n",
      "762: man-AI Systems: A Literature  Meta-Review  Synopsis of Key Ideas and Publications  \n",
      "763: and Bibliography  for Explainable  AI, Technical  Report, Defense Advanced  Research  \n",
      "764: Projects Agency (DARPA)  XAI Program,  2019 . \n",
      "765: of methods for explaining  black box models, ACM Computing  Surveys 51 (5) (2018) \n",
      "766: ary fuzzy systems for explainable  artiﬁcial intelligence:  Why, when, what for, and \n",
      "767: [21] M.W. Craven , Extracting  comprehensible  models from trained neural networks,  \n",
      "768: [24] D. Doran, S. Schulz, T.R. Besold, What does explainable  AI really mean? a new \n",
      "769: [29] F. Rossi, AI Ethics for Enterprise  AI, 2019. \n",
      "770: plainable  Ai systems for the medical domain?,  2017. \n",
      "771: SAIL-TR-2015-010,  2015 . \n",
      "772: [32] M.T. Ribeiro , S. Singh , C. Guestrin , Why should I trust you?: Explaining  the pre- \n",
      "773: [33] M. Fox, D. Long, D. Magazzeni,  Explainable  planning,  2017. \n",
      "774: [34] H.C. Lane , M.G. Core , M. Van Lent , S. Solomon , D. Gomboc , Explainable  artiﬁcial \n",
      "775: intelligence  for training and tutoring, Technical  Report, University  of Southern  \n",
      "776: [37] A. Chander , R. Srinivasan  , S. Chelian , J. Wang , K. Uchino , Working with beliefs: AI \n",
      "777: rections and challenges  in extracting  the knowledge  embedded  within trained arti- \n",
      "778: [40] O. Goudet , D. Kalainathan  , P. Caillou , I. Guyon , D. Lopez-Paz  , M. Sebag , Learn- \n",
      "779: ing functional  causal models with generative  neural networks,  in: Explainable  and \n",
      "780: [43] C. Barabas, K. Dinakar, J. Ito, M. Virza, J. Zittrain, Interventions  over predictions:  \n",
      "781: [46] W. Samek, T. Wiegand,  K.-R. Müller, Explainable  artiﬁcial intelligence:  Under- \n",
      "782: standing,  visualizing  and interpreting  deep learning models, 2017. \n",
      "783: [47] C. Wadsworth,  F. Vera, C. Piech, Achieving  fairness through adversarial  learning: \n",
      "784: deep learning, IEEE Transactions  on Neural Networks  and Learning Systems 30 \n",
      "785: [50] M. Harbers , K. van den Bosch , J.-J. Meyer , Design and evaluation  of explainable  \n",
      "786: son , T.C. Bailey , A. Hernandez  , Conﬁdent  interpretation  of bayesian decision tree \n",
      "787: [58] F.J.C. Garcia, D.A. Robb, X. Liu, A. Laskov, P. Patron, H. Hastie, Explain yourself: \n",
      "788: [59] P. Langley , B. Meadows  , M. Sridharan  , D. Choi , Explainable  agency for intelli- \n",
      "789: gent autonomous  systems, in: AAAI Conference  on Artiﬁcial Intelligence,  2017, \n",
      "790: [60] G. Montavon  , S. Lapuschkin  , A. Binder , W. Samek , K.-R. Müller , Explaining  non- \n",
      "791: Learning how to explain neural networks:  Patternnet  and patternattribution,  2017. \n",
      "792: [62] G. Ras , M. van Gerven , P. Haselager  , Explanation  methods in deep learning: Users, \n",
      "793: values, concerns and challenges,  in: Explainable  and Interpretable  Models in Com- \n",
      "794: autoencoders,  in: Paciﬁc Symposium  on Biocomputing  Co-Chairs,  World Scientiﬁc,  \n",
      "795: trained neural networks:  a practical and eﬃcient approach,  IEEE Transactions  on \n",
      "796: sity: Tree regularization  of deep models for interpretability,  in: AAAI Conference  \n",
      "797: [83] Z.-H. Zhou , Y. Jiang , S.-F. Chen , Extracting  symbolic  rules from trained neural \n",
      "798: network ensembles,  AI Communications  16 (1) (2003) 3–15 . \n",
      "799: [86] T. Miller , P. Howe , L. Sonenberg  , Explainable  AI: Beware of inmates running the \n",
      "800: Explainable  AI (XAI), 36, 2017, pp. 36–40 . \n",
      "801: berg , A. Holzinger  , Explainable  AI: the new 42? in: International  Cross-Domain  \n",
      "802: [88] V. Belle , Logic meets probability:  Towards explainable  AI systems for uncer- \n",
      "803: tain worlds, in: International  Joint Conference  on Artiﬁcial Intelligence,  2017, \n",
      "804: [90] Y. Lou , R. Caruana , J. Gehrke , G. Hooker , Accurate intelligible  models with pair- \n",
      "805: [97] R. Krishnan  , G. Sivakumar  , P. Bhattacharya  , Extracting  decision trees from trained \n",
      "806: [99] B. Green , “Fair ”r i s k assessments:  A precarious  approach  for criminal justice re- \n",
      "807: form, in: 5th Workshop  on Fairness, Accountability,  and Transparency  in Machine \n",
      "808: [100] A. Chouldechova  , Fair prediction  with disparate  impact: A study of bias in recidi- \n",
      "809: [101] M. Kim , O. Reingold  , G. Rothblum  , Fairness through computationally-bounded  \n",
      "810: [106] Y. Zhang , H. Su , T. Jia , J. Chu , Rule extraction  from trained support vector ma- \n",
      "811: models using transparent  model distillation,  in: AAAI/ACM  Conference  on AI, \n",
      "812: [128] M. Kearns, S. Neel, A. Roth, Z.S. Wu, Preventing  fairness gerrymandering:  Auditing \n",
      "813: and learning for subgroup  fairness, 2017. \n",
      "814: [135] J.J. Thiagarajan,  B. Kailkhura,  P. Sattigeri,  K.N. Ramamurthy,  Treeview:  Peeking \n",
      "815: pretable and ﬁne-grained  visual explanations  for convolutional  neural networks,  \n",
      "816: [139] A. Kanehira  , T. Harada , Learning to explain with complemental  examples,  in: Pro- \n",
      "817: The all convolutional  net, 2014. [144] B. Kim, M. Wattenberg,  J. Gilmer, C. Cai, J. Wexler, F. Viegas, R. Sayres, Inter- \n",
      "818: trained neural networks,  in: Machine learning proceedings  1994, Elsevier, 1994, \n",
      "819: [148] A.D. Arbatli , H.L. Akin , Rule extraction  from trained neural networks  using genetic \n",
      "820: [155] P.E. Pope , S. Kolouri , M. Rostami , C.E. Martin , H. Hoﬀmann  , Explainability  meth- \n",
      "821: [156] P. Gajane, M. Pechenizkiy,  On formalizing  fairness in prediction  with machine \n",
      "822: [157] C. Dwork, C. Ilvento, Composition  of fairsystems,  2018. \n",
      "823: [158] S. Barocas, M. Hardt, A. Narayanan,  Fairness and Machine Learning,  fairml- \n",
      "824: book.org,  2019. http://www.fairmlbook.org  \n",
      "825: [169] A. Bennetot  , J.-L. Laurent , R. Chatila , N. Díaz-Rodríguez  , Towards explainable  neu- \n",
      "826: ral-symbolic  visual reasoning,  in: NeSy Workshop  IJCAI 2019, Macau, China, 2019 . \n",
      "827: [172] K. Kawaguchi  , Deep learning without poor local minima, in: Advances  in neural \n",
      "828: [183] L. Rokach , O.Z. Maimon , Data mining with decision trees: theory and applications,  \n",
      "829: opaque models using genetic programming.,  in: FLAIRS Conference,  Miami Beach, \n",
      "830: [191] J.R. Quinlan , Generating  production  rules from decision trees., in: ijcai, 87, Cite- \n",
      "831: factors and distributions  of pelagic ﬁsh and krill: a case study in sendai bay, Japan, \n",
      "832: [207] A.R. Cassandra  , L.P. Kaelbling  , J.A. Kurien , Acting under uncertainty:  Discrete \n",
      "833: tions by identifying  prediction  invariance,  2016. [217] M.W. Craven , Extracting  Comprehensible  Models from Trained Neural Networks,  \n",
      "834: 1996 Ph.D. thesis . AAI9700774  \n",
      "835: [226] H. Chen, S. Lundberg,  S.-I. Lee, Explaining  models by propagating  shapley values \n",
      "836: [229] J. Moeyersoms,  B. d’Alessandro,  F. Provost, D. Martens, Explaining  classiﬁcation  \n",
      "837: How to explain individual  classiﬁcation  decisions,  Journal of Machine Learning \n",
      "838: [234] M. Robnik- Š ikonja , I. Kononenko  , Explaining  classiﬁcations  for individual  in- \n",
      "839: planations,  in: AAAI Conference  on Artiﬁcial Intelligence,  2018, pp. 1527–\n",
      "840: [236] D. Martens , F. Provost , Explaining  data-driven  document  classiﬁcations,  MIS Quar- \n",
      "841: [237] D. Chen , S.P. Fraiberger  , R. Moakler , F. Provost , Enhancing  transparency  and con- \n",
      "842: [240] G. Tolomei , F. Silvestri , A. Haines , M. Lalmas , Interpretable  predictions  of \n",
      "843: [243] N.F. Rajani , R.J. Mooney , Ensembling  visual explanations,  in: Explainable  and In- \n",
      "844: [256] H. Tsukimoto  , Extracting  rules from trained neural networks,  IEEE Transactions  on \n",
      "845: [260] R. Féraud , F. Clérot , A methodology  to explain neural network classiﬁcation,  Neural \n",
      "846: pretable and robust deep learning, 2018. \n",
      "847: [271] Y. Goyal, A. Mohapatra,  D. Parikh, D. Batra, Towards transparent  AI systems: In- \n",
      "848: attention  models in deep convolutional  neural network for ﬁne-grained  image clas- \n",
      "849: [280] L. Arras, G. Montavon,  K.-R. Müller, W. Samek, Explaining  recurrent  neural net- \n",
      "850: [282] J. Clos , N. Wiratunga  , S. Massie , Towards explainable  text classiﬁcation  by jointly \n",
      "851: learning lexicon and modiﬁer terms, in: IJCAI-17 Workshop  on Explainable  AI \n",
      "852: (XAI), 2017, p. 19 . \n",
      "853: [285] E. Choi , M.T. Bahadori  , J. Sun , J. Kulas , A. Schuetz , W. Stewart , Retain: An inter- pretable predictive  model for healthcare  using reverse time attention  mechanism,  \n",
      "854: [287] A. Lucic, H. Haned, M. de Rijke, Explaining  predictions  from tree-based  boosting \n",
      "855: [290] R. Traoré, H. Caselles-Dupré,  T. Lesort, T. Sun, G. Cai, N.D. Rodríguez,  D. Filliat, \n",
      "856: works with applications  to healthcare  domain, 2015. \n",
      "857: Artiﬁcial Intelligence,  IJCAI (2017) 1596–1602  . \n",
      "858: on dietary data, in: First Workshop  on Semantic  Explainability  @ ISWC 2019, 2019 . \n",
      "859: ence on Artiﬁcial Intelligence,  IJCAI-18,  2018, pp. 1362–1368  . \n",
      "860: [311] M. Zolotas , Y. Demiris , in: Towards explainable  shared control using augmented  \n",
      "861: toencoders  for explainable  recommender  systems, in: Proceedings  of the 3rd Work- \n",
      "862: shop on Deep Learning for Recommender  Systems, in: DLRS 2018, 2018, pp. 24–31 . \n",
      "863: [314] C.-Z. A. Huang, A. Vaswani,  J. Uszkoreit,  N. Shazeer, C. Hawthorne,  A.M. Dai, M.D. \n",
      "864: [315] M. Cornia, L. Baraldi, R. Cucchiara,  Smart: Training shallow memory-aware  trans- \n",
      "865: formers for robotic explainability,  2019. \n",
      "866: XAI: An Overview  of ANN-CBR  Twins for Explaining  Deep Learning,  2019. \n",
      "867: [319] T. Hailesilassie,  Rule extraction  algorithm  for deep neural networks:  A review, \n",
      "868: [328] Y. Zhang, X. Chen, Explainable  Recommendation:  A Survey and New Perspectives,  \n",
      "869: [329] J. Frankle, M. Carbin, The Lottery Ticket Hypothesis:  Finding Sparse, Trainable  \n",
      "870: [330] A. Vaswani,  N. Shazeer, N. Parmar, J. Uszkoreit,  L. Jones, A.N. Gomez, L. Kaiser, \n",
      "871: [334] A. Slavin Ross, M.C. Hughes, F. Doshi-Velez,  Right for the Right Reasons: Training \n",
      "872: Diﬀerentiable  Models by Constraining  their Explanations,  2017. \n",
      "873: A. Lerchner , beta-vae:  Learning basic visual concepts with a constrained  variational  \n",
      "874: Networks  to Explain Neural Networks,  2018. \n",
      "875: [345] D. Bouchacourt,  L. Denoyer,  EDUCE: explaining  model decisions  through unsuper- \n",
      "876: [347] C. Rudin, Please stop explaining  black box models for high stakes decisions,  2018. \n",
      "877: [349] R.R. Hoﬀman,  S.T. Mueller, G. Klein, J. Litman, Metrics for explainable  ai: Chal- \n",
      "878: design and evaluation  of explainable  ai systems, 2018. \n",
      "879: [351] R.M.J. Byrne , Counterfactuals  in explainable  artiﬁcial intelligence  (XAI): Evidence  \n",
      "880: Conference  on Artiﬁcial Intelligence,  IJCAI-19,  2019, pp. 6276–6282  . \n",
      "881: [352] M. Garnelo , M. Shanahan  , Reconciling  deep learning with symbolic  artiﬁcial intel- \n",
      "882: works, in: Explainable  AI: Interpreting,  Explaining  and Visualizing  Deep Learning,  \n",
      "883: Springer,  2019, pp. 121–144 . [358] I.J. Goodfellow,  J. Shlens, C. Szegedy, Explaining  and harnessing  adversarial  ex- \n",
      "884: Kohno, D. Song, Robust physical-world  attacks on deep learning models, 2017. \n",
      "885: [362] B. Biggio , I. Corona , D. Maiorca , B. Nelson , N. Š rndi ć, P. Laskov , G. Giacinto , F. Roli , \n",
      "886: Evasion attacks against machine learning at test time, in: Proceedings  of the 2013th \n",
      "887: [363] B. Biggio, I. Pillai, S.R. Bulò, D. Ariu, M. Pelillo, F. Roli, Is data clustering  in adver- \n",
      "888: [367] C. Biﬃ, O. Oktay , G. Tarroni , W. Bai , A. De Marvao , G. Doumou , M. Rajchl , \n",
      "889: R. Bedair , S. Prasad , S. Cook , et al. , Learning interpretable  anatomical  features \n",
      "890: [368] S. Liu, B. Kailkhura,  D. Loveland,  Y. Han, Generative  counterfactual  introspection  \n",
      "891: for explainable  deep learning, 2019. \n",
      "892: [375] G. Hautier , C.C. Fischer , A. Jain , T. Mueller , G. Ceder , Finding nature’s missing \n",
      "893: gap-ﬁlling  and trait prediction  for macroecology  and functional  biogeography,  \n",
      "894: [384] C. Rudin , Stop explaining  black box machine learning models for high stakes deci- \n",
      "895: [386] R. Benjamins,  A. Barbado,  D. Sierra, Responsible  AI by design, 2019. \n",
      "896: [387] United-Nations  , Transforming  our World: the 2030 Agenda for Sustainable  Devel- \n",
      "897: [389] B.C. Stahl , D. Wright , Ethics and privacy in ai and big data: Implementing  respon- \n",
      "898: thy AI, Technical  Report, European  Commission,  2019 . \n",
      "899: M.B. Zafar , A uniﬁed approach  to quantifying  algorithmic  unfairness:  Measuring  \n",
      "900: individual  group unfairness  via inequality  indices, in: Proceedings  of the 24th ACM \n",
      "901: [396] R. Zemel , Y. Wu , K. Swersky , T. Pitassi , C. Dwork , Learning fair representations,  \n",
      "902: learning, in: Proceedings  of the 2018 AAAI/ACM  Conference  on AI, Ethics, and \n",
      "903: [398] Y. Ahn , Y.-R. Lin , Fairsight:  Visual analytics for fairness in decision making, IEEE \n",
      "904: [399] E. Soares , P. Angelov , Fair-by-design  explainable  models for prediction  of recidi- \n",
      "905: [400] J. Dressel , H. Farid , The accuracy,  fairness, and limits of predicting  recidivism,  \n",
      "906: [401] U. Aivodji , H. Arai , O. Fortineau  , S. Gambs , S. Hara , A. Tapp , Fairwashing:  the \n",
      "907: [402] S. Sharma , J. Henderson  , J. Ghosh , Certifai: Counterfactual  explanations  for robust- \n",
      "908: ness, transparency,  interpretability,  and fairness of artiﬁcial intelligence  models, \n",
      "909: ference on Fairness, Accountability,  and Transparency,  ACM, 2019, pp. 220–229 . \n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc = docx.Document(docx_path)\n",
    "print(f\"Number of paragraphs: {len(doc.paragraphs)}\")\n",
    "for count, para in enumerate(doc.paragraphs, start=1):\n",
    "    print(f\"{count}: {para.text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
